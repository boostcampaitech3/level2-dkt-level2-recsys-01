{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yt7sDAqHhfQp"
      },
      "source": [
        "# LGBM을 활용한 베이스라인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-05-24T09:49:29.375544Z",
          "start_time": "2021-05-24T09:49:28.999092Z"
        },
        "id": "Uq_TJqbdhfQu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZlm5HSmhfQv"
      },
      "source": [
        "## 1. 데이터 로딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-05-24T09:49:29.678737Z",
          "start_time": "2021-05-24T09:49:29.376581Z"
        },
        "id": "s6qgJ8MLhfQw"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user</th>\n",
              "      <th>assessmentItemID</th>\n",
              "      <th>testId</th>\n",
              "      <th>answer</th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>tag</th>\n",
              "      <th>label</th>\n",
              "      <th>category</th>\n",
              "      <th>test</th>\n",
              "      <th>item</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>weekday</th>\n",
              "      <th>hour</th>\n",
              "      <th>elapsed</th>\n",
              "      <th>test_elapsed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>A060001001</td>\n",
              "      <td>A060000001</td>\n",
              "      <td>1</td>\n",
              "      <td>2020-03-24 00:17:11</td>\n",
              "      <td>7224</td>\n",
              "      <td>train</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>A060001002</td>\n",
              "      <td>A060000001</td>\n",
              "      <td>1</td>\n",
              "      <td>2020-03-24 00:17:14</td>\n",
              "      <td>7225</td>\n",
              "      <td>train</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>A060001003</td>\n",
              "      <td>A060000001</td>\n",
              "      <td>1</td>\n",
              "      <td>2020-03-24 00:17:22</td>\n",
              "      <td>7225</td>\n",
              "      <td>train</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>A060001004</td>\n",
              "      <td>A060000001</td>\n",
              "      <td>1</td>\n",
              "      <td>2020-03-24 00:17:29</td>\n",
              "      <td>7225</td>\n",
              "      <td>train</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>A060001005</td>\n",
              "      <td>A060000001</td>\n",
              "      <td>1</td>\n",
              "      <td>2020-03-24 00:17:36</td>\n",
              "      <td>7225</td>\n",
              "      <td>train</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>11.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2526695</th>\n",
              "      <td>7441</td>\n",
              "      <td>A030071005</td>\n",
              "      <td>A030000071</td>\n",
              "      <td>0</td>\n",
              "      <td>2020-06-05 06:50:21</td>\n",
              "      <td>438</td>\n",
              "      <td>train</td>\n",
              "      <td>3</td>\n",
              "      <td>71</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>65778.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2526696</th>\n",
              "      <td>7441</td>\n",
              "      <td>A040165001</td>\n",
              "      <td>A040000165</td>\n",
              "      <td>1</td>\n",
              "      <td>2020-08-21 01:06:39</td>\n",
              "      <td>8836</td>\n",
              "      <td>train</td>\n",
              "      <td>4</td>\n",
              "      <td>165</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>21</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>11.0</td>\n",
              "      <td>11.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2526697</th>\n",
              "      <td>7441</td>\n",
              "      <td>A040165002</td>\n",
              "      <td>A040000165</td>\n",
              "      <td>1</td>\n",
              "      <td>2020-08-21 01:06:50</td>\n",
              "      <td>8836</td>\n",
              "      <td>train</td>\n",
              "      <td>4</td>\n",
              "      <td>165</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>21</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>46.0</td>\n",
              "      <td>46.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2526698</th>\n",
              "      <td>7441</td>\n",
              "      <td>A040165003</td>\n",
              "      <td>A040000165</td>\n",
              "      <td>1</td>\n",
              "      <td>2020-08-21 01:07:36</td>\n",
              "      <td>8836</td>\n",
              "      <td>train</td>\n",
              "      <td>4</td>\n",
              "      <td>165</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>21</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>73.0</td>\n",
              "      <td>73.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2526699</th>\n",
              "      <td>7441</td>\n",
              "      <td>A040165004</td>\n",
              "      <td>A040000165</td>\n",
              "      <td>1</td>\n",
              "      <td>2020-08-21 01:08:49</td>\n",
              "      <td>8836</td>\n",
              "      <td>train</td>\n",
              "      <td>4</td>\n",
              "      <td>165</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>21</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2526700 rows × 16 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         user assessmentItemID      testId  answer            Timestamp   tag  \\\n",
              "0           0       A060001001  A060000001       1  2020-03-24 00:17:11  7224   \n",
              "1           0       A060001002  A060000001       1  2020-03-24 00:17:14  7225   \n",
              "2           0       A060001003  A060000001       1  2020-03-24 00:17:22  7225   \n",
              "3           0       A060001004  A060000001       1  2020-03-24 00:17:29  7225   \n",
              "4           0       A060001005  A060000001       1  2020-03-24 00:17:36  7225   \n",
              "...       ...              ...         ...     ...                  ...   ...   \n",
              "2526695  7441       A030071005  A030000071       0  2020-06-05 06:50:21   438   \n",
              "2526696  7441       A040165001  A040000165       1  2020-08-21 01:06:39  8836   \n",
              "2526697  7441       A040165002  A040000165       1  2020-08-21 01:06:50  8836   \n",
              "2526698  7441       A040165003  A040000165       1  2020-08-21 01:07:36  8836   \n",
              "2526699  7441       A040165004  A040000165       1  2020-08-21 01:08:49  8836   \n",
              "\n",
              "         label  category  test  item  month  day  weekday  hour  elapsed  \\\n",
              "0        train         6     1     1      3   24        1     0      3.0   \n",
              "1        train         6     1     2      3   24        1     0      8.0   \n",
              "2        train         6     1     3      3   24        1     0      7.0   \n",
              "3        train         6     1     4      3   24        1     0      7.0   \n",
              "4        train         6     1     5      3   24        1     0     11.0   \n",
              "...        ...       ...   ...   ...    ...  ...      ...   ...      ...   \n",
              "2526695  train         3    71     5      6    5        4     6  65778.0   \n",
              "2526696  train         4   165     1      8   21        4     1     11.0   \n",
              "2526697  train         4   165     2      8   21        4     1     46.0   \n",
              "2526698  train         4   165     3      8   21        4     1     73.0   \n",
              "2526699  train         4   165     4      8   21        4     1      NaN   \n",
              "\n",
              "         test_elapsed  \n",
              "0                 3.0  \n",
              "1                 8.0  \n",
              "2                 7.0  \n",
              "3                 7.0  \n",
              "4                11.0  \n",
              "...               ...  \n",
              "2526695           NaN  \n",
              "2526696          11.0  \n",
              "2526697          46.0  \n",
              "2526698          73.0  \n",
              "2526699           NaN  \n",
              "\n",
              "[2526700 rows x 16 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df = pd.read_csv(\"/opt/ml/input/data/preprocessed_data.csv\")\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_oCGAgEhfQw"
      },
      "source": [
        "## 2. Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-05-24T09:49:29.682739Z",
          "start_time": "2021-05-24T09:49:28.979Z"
        },
        "id": "URNLoukChfQx"
      },
      "outputs": [],
      "source": [
        "def feature_engineering(df):\n",
        "    \n",
        "    #유저별 시퀀스를 고려하기 위해 아래와 같이 정렬\n",
        "    df.sort_values(by=['user','Timestamp'], inplace=True)\n",
        "    \n",
        "    #유저들의 문제 풀이수, 정답 수, 정답률을 시간순으로 누적해서 계산\n",
        "    df['user_correct_answer'] = df.groupby('user')['answer'].transform(lambda x: x.cumsum().shift(1))\n",
        "    df['user_total_answer'] = df.groupby('user')['answer'].cumcount()\n",
        "    df['user_acc'] = df['user_correct_answer']/df['user_total_answer']\n",
        "\n",
        "    #유저별 카테고리별\n",
        "    df['user_category_correct_answer'] = df.groupby(['user', 'category'])['answer'].transform(lambda x: x.cumsum().shift(1))\n",
        "    df['user_category_total_answer'] = df.groupby(['user', 'category'])['answer'].cumcount()\n",
        "    df['user_category_acc'] = df['user_category_correct_answer']/df['user_category_total_answer']\n",
        "\n",
        "    #유저별 태그별\n",
        "    df['user_tag_correct_answer'] = df.groupby(['user', 'tag'])['answer'].transform(lambda x: x.cumsum().shift(1))\n",
        "    df['user_tag_total_answer'] = df.groupby(['user', 'tag'])['answer'].cumcount()\n",
        "    df['user_tag_acc'] = df['user_tag_correct_answer']/df['user_tag_total_answer']\n",
        "\n",
        "    # testId와 KnowledgeTag의 전체 정답률은 한번에 계산\n",
        "    # 아래 데이터는 제출용 데이터셋에 대해서도 재사용\n",
        "    correct_t = df.groupby(['testId'])['answer'].agg(['mean', 'sum'])\n",
        "    correct_t.columns = [\"test_mean\", 'test_sum']\n",
        "    correct_k = df.groupby(['tag'])['answer'].agg(['mean', 'sum'])\n",
        "    correct_k.columns = [\"tag_mean\", 'tag_sum']\n",
        "\n",
        "    elapsed_user = df.groupby(['user'])['elapsed'].mean()\n",
        "    elapsed_user.rename('elapsed_user', inplace=True)\n",
        "    test_elapsed_user = df.groupby(['user'])['test_elapsed'].mean()\n",
        "    test_elapsed_user.rename('test_elapsed_user', inplace=True)\n",
        "    elapsed_testId = df.groupby(['testId'])['elapsed'].mean()\n",
        "    elapsed_testId.rename('elapsed_testId', inplace=True)\n",
        "    test_elapsed_testId = df.groupby(['testId'])['test_elapsed'].mean()\n",
        "    test_elapsed_testId.rename('test_elapsed_testId', inplace=True)\n",
        "    elapsed_category = df.groupby(['category'])['elapsed'].mean()\n",
        "    elapsed_category.rename('elapsed_category', inplace=True)\n",
        "    test_elapsed_category = df.groupby(['category'])['test_elapsed'].mean()\n",
        "    test_elapsed_category.rename('test_elapsed_category', inplace=True)\n",
        "    elapsed_user_testId = df.groupby(['user', 'testId'])['elapsed'].mean()\n",
        "    elapsed_user_testId.rename('elapsed_user_testId', inplace=True)\n",
        "    test_elapsed_user_testId = df.groupby(['user', 'testId'])['test_elapsed'].mean()\n",
        "    test_elapsed_user_testId.rename('test_elapsed_user_testId', inplace=True)\n",
        "    elapsed_user_category = df.groupby(['user', 'category'])['elapsed'].mean()\n",
        "    elapsed_user_category.rename('elapsed_user_category', inplace=True)\n",
        "    test_elapsed_user_category = df.groupby(['user', 'category'])['test_elapsed'].mean()\n",
        "    test_elapsed_user_category.rename('test_elapsed_user_category', inplace=True)\n",
        "    elapsed_assessmentItemID = df.groupby(['assessmentItemID'])['elapsed'].mean()\n",
        "    elapsed_assessmentItemID.rename('elapsed_assessmentItemID', inplace=True)\n",
        "    test_elapsed_assessmentItemID = df.groupby(['assessmentItemID'])['test_elapsed'].mean()\n",
        "    test_elapsed_assessmentItemID.rename('test_elapsed_assessmentItemID', inplace=True)\n",
        "    elapsed_tag = df.groupby(['tag'])['elapsed'].mean()\n",
        "    elapsed_tag.rename('elapsed_tag', inplace=True)\n",
        "    test_elapsed_tag = df.groupby(['tag'])['test_elapsed'].mean()\n",
        "    test_elapsed_tag.rename('test_elapsed_tag', inplace=True)\n",
        "    elapsed_user_tag = df.groupby(['user', 'tag'])['elapsed'].mean()\n",
        "    elapsed_user_tag.rename('elapsed_user_tag', inplace=True)\n",
        "    test_elapsed_user_tag = df.groupby(['user', 'tag'])['test_elapsed'].mean()\n",
        "    test_elapsed_user_tag.rename('test_elapsed_user_tag', inplace=True)\n",
        "\n",
        "    df = pd.merge(df, correct_t, on=['testId'], how=\"left\")\n",
        "    df = pd.merge(df, correct_k, on=['tag'], how=\"left\")\n",
        "    df = pd.merge(df, elapsed_user, on=['user'], how=\"left\")\n",
        "    df = pd.merge(df, test_elapsed_user, on=['user'], how=\"left\")\n",
        "    df = pd.merge(df, elapsed_testId, on=['testId'], how=\"left\")\n",
        "    df = pd.merge(df, test_elapsed_testId, on=['testId'], how=\"left\")\n",
        "    df = pd.merge(df, elapsed_category, on=['category'], how=\"left\")\n",
        "    df = pd.merge(df, test_elapsed_category, on=['category'], how=\"left\")\n",
        "    df = pd.merge(df, elapsed_user_testId, on=['user', 'testId'], how=\"left\")\n",
        "    df = pd.merge(df, test_elapsed_user_testId, on=['user', 'testId'], how=\"left\")\n",
        "    df = pd.merge(df, elapsed_user_category, on=['user', 'category'], how=\"left\")\n",
        "    df = pd.merge(df, test_elapsed_user_category, on=['user', 'category'], how=\"left\")\n",
        "    df = pd.merge(df, elapsed_assessmentItemID, on=['assessmentItemID'], how=\"left\")\n",
        "    df = pd.merge(df, test_elapsed_assessmentItemID, on=['assessmentItemID'], how=\"left\")\n",
        "    df = pd.merge(df, elapsed_tag, on=['tag'], how=\"left\")\n",
        "    df = pd.merge(df, test_elapsed_tag, on=['tag'], how=\"left\")\n",
        "    df = pd.merge(df, elapsed_user_tag, on=['user', 'tag'], how=\"left\")\n",
        "    df = pd.merge(df, test_elapsed_user_tag, on=['user', 'tag'], how=\"left\")\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-05-24T09:49:29.683739Z",
          "start_time": "2021-05-24T09:49:28.981Z"
        },
        "id": "2vsUwksMhfQy",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "df = feature_engineering(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user</th>\n",
              "      <th>assessmentItemID</th>\n",
              "      <th>testId</th>\n",
              "      <th>answer</th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>tag</th>\n",
              "      <th>label</th>\n",
              "      <th>category</th>\n",
              "      <th>test</th>\n",
              "      <th>item</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>weekday</th>\n",
              "      <th>hour</th>\n",
              "      <th>elapsed</th>\n",
              "      <th>test_elapsed</th>\n",
              "      <th>user_correct_answer</th>\n",
              "      <th>user_total_answer</th>\n",
              "      <th>user_acc</th>\n",
              "      <th>user_category_correct_answer</th>\n",
              "      <th>user_category_total_answer</th>\n",
              "      <th>user_category_acc</th>\n",
              "      <th>user_tag_correct_answer</th>\n",
              "      <th>user_tag_total_answer</th>\n",
              "      <th>user_tag_acc</th>\n",
              "      <th>test_mean</th>\n",
              "      <th>test_sum</th>\n",
              "      <th>tag_mean</th>\n",
              "      <th>tag_sum</th>\n",
              "      <th>elapsed_user</th>\n",
              "      <th>test_elapsed_user</th>\n",
              "      <th>elapsed_testId</th>\n",
              "      <th>test_elapsed_testId</th>\n",
              "      <th>elapsed_category</th>\n",
              "      <th>test_elapsed_category</th>\n",
              "      <th>elapsed_user_testId</th>\n",
              "      <th>test_elapsed_user_testId</th>\n",
              "      <th>elapsed_user_category</th>\n",
              "      <th>test_elapsed_user_category</th>\n",
              "      <th>elapsed_assessmentItemID</th>\n",
              "      <th>test_elapsed_assessmentItemID</th>\n",
              "      <th>elapsed_tag</th>\n",
              "      <th>test_elapsed_tag</th>\n",
              "      <th>elapsed_user_tag</th>\n",
              "      <th>test_elapsed_user_tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>A060001001</td>\n",
              "      <td>A060000001</td>\n",
              "      <td>1</td>\n",
              "      <td>2020-03-24 00:17:11</td>\n",
              "      <td>7224</td>\n",
              "      <td>train</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.952667</td>\n",
              "      <td>1429</td>\n",
              "      <td>0.957333</td>\n",
              "      <td>718</td>\n",
              "      <td>6171.22043</td>\n",
              "      <td>50.592824</td>\n",
              "      <td>7022.485333</td>\n",
              "      <td>41.259585</td>\n",
              "      <td>5951.348344</td>\n",
              "      <td>231.106563</td>\n",
              "      <td>3348.666667</td>\n",
              "      <td>7.200000</td>\n",
              "      <td>7150.878613</td>\n",
              "      <td>44.469595</td>\n",
              "      <td>13.660000</td>\n",
              "      <td>13.660000</td>\n",
              "      <td>108.744000</td>\n",
              "      <td>19.895861</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>A060001002</td>\n",
              "      <td>A060000001</td>\n",
              "      <td>1</td>\n",
              "      <td>2020-03-24 00:17:14</td>\n",
              "      <td>7225</td>\n",
              "      <td>train</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.952667</td>\n",
              "      <td>1429</td>\n",
              "      <td>0.917067</td>\n",
              "      <td>3439</td>\n",
              "      <td>6171.22043</td>\n",
              "      <td>50.592824</td>\n",
              "      <td>7022.485333</td>\n",
              "      <td>41.259585</td>\n",
              "      <td>5951.348344</td>\n",
              "      <td>231.106563</td>\n",
              "      <td>3348.666667</td>\n",
              "      <td>7.200000</td>\n",
              "      <td>7150.878613</td>\n",
              "      <td>44.469595</td>\n",
              "      <td>26.112000</td>\n",
              "      <td>26.112000</td>\n",
              "      <td>7416.078667</td>\n",
              "      <td>113.686398</td>\n",
              "      <td>4017.800000</td>\n",
              "      <td>8.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>A060001003</td>\n",
              "      <td>A060000001</td>\n",
              "      <td>1</td>\n",
              "      <td>2020-03-24 00:17:22</td>\n",
              "      <td>7225</td>\n",
              "      <td>train</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.952667</td>\n",
              "      <td>1429</td>\n",
              "      <td>0.917067</td>\n",
              "      <td>3439</td>\n",
              "      <td>6171.22043</td>\n",
              "      <td>50.592824</td>\n",
              "      <td>7022.485333</td>\n",
              "      <td>41.259585</td>\n",
              "      <td>5951.348344</td>\n",
              "      <td>231.106563</td>\n",
              "      <td>3348.666667</td>\n",
              "      <td>7.200000</td>\n",
              "      <td>7150.878613</td>\n",
              "      <td>44.469595</td>\n",
              "      <td>19.180000</td>\n",
              "      <td>19.180000</td>\n",
              "      <td>7416.078667</td>\n",
              "      <td>113.686398</td>\n",
              "      <td>4017.800000</td>\n",
              "      <td>8.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>A060001004</td>\n",
              "      <td>A060000001</td>\n",
              "      <td>1</td>\n",
              "      <td>2020-03-24 00:17:29</td>\n",
              "      <td>7225</td>\n",
              "      <td>train</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1.00</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.952667</td>\n",
              "      <td>1429</td>\n",
              "      <td>0.917067</td>\n",
              "      <td>3439</td>\n",
              "      <td>6171.22043</td>\n",
              "      <td>50.592824</td>\n",
              "      <td>7022.485333</td>\n",
              "      <td>41.259585</td>\n",
              "      <td>5951.348344</td>\n",
              "      <td>231.106563</td>\n",
              "      <td>3348.666667</td>\n",
              "      <td>7.200000</td>\n",
              "      <td>7150.878613</td>\n",
              "      <td>44.469595</td>\n",
              "      <td>18.076000</td>\n",
              "      <td>18.076000</td>\n",
              "      <td>7416.078667</td>\n",
              "      <td>113.686398</td>\n",
              "      <td>4017.800000</td>\n",
              "      <td>8.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>A060001005</td>\n",
              "      <td>A060000001</td>\n",
              "      <td>1</td>\n",
              "      <td>2020-03-24 00:17:36</td>\n",
              "      <td>7225</td>\n",
              "      <td>train</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4</td>\n",
              "      <td>1.00</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.952667</td>\n",
              "      <td>1429</td>\n",
              "      <td>0.917067</td>\n",
              "      <td>3439</td>\n",
              "      <td>6171.22043</td>\n",
              "      <td>50.592824</td>\n",
              "      <td>7022.485333</td>\n",
              "      <td>41.259585</td>\n",
              "      <td>5951.348344</td>\n",
              "      <td>231.106563</td>\n",
              "      <td>3348.666667</td>\n",
              "      <td>7.200000</td>\n",
              "      <td>7150.878613</td>\n",
              "      <td>44.469595</td>\n",
              "      <td>35.720000</td>\n",
              "      <td>35.720000</td>\n",
              "      <td>7416.078667</td>\n",
              "      <td>113.686398</td>\n",
              "      <td>4017.800000</td>\n",
              "      <td>8.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2526695</th>\n",
              "      <td>7441</td>\n",
              "      <td>A030071005</td>\n",
              "      <td>A030000071</td>\n",
              "      <td>0</td>\n",
              "      <td>2020-06-05 06:50:21</td>\n",
              "      <td>438</td>\n",
              "      <td>train</td>\n",
              "      <td>3</td>\n",
              "      <td>71</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>65778.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.666000</td>\n",
              "      <td>999</td>\n",
              "      <td>0.694889</td>\n",
              "      <td>3127</td>\n",
              "      <td>8266.00000</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>7818.508339</td>\n",
              "      <td>388.870539</td>\n",
              "      <td>7109.351951</td>\n",
              "      <td>284.402511</td>\n",
              "      <td>13199.600000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>13199.600000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>38109.581940</td>\n",
              "      <td>33704.500000</td>\n",
              "      <td>7430.302067</td>\n",
              "      <td>237.235946</td>\n",
              "      <td>13199.600000</td>\n",
              "      <td>55.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2526696</th>\n",
              "      <td>7441</td>\n",
              "      <td>A040165001</td>\n",
              "      <td>A040000165</td>\n",
              "      <td>1</td>\n",
              "      <td>2020-08-21 01:06:39</td>\n",
              "      <td>8836</td>\n",
              "      <td>train</td>\n",
              "      <td>4</td>\n",
              "      <td>165</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>21</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>11.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.652500</td>\n",
              "      <td>783</td>\n",
              "      <td>0.698551</td>\n",
              "      <td>2410</td>\n",
              "      <td>8266.00000</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>8837.122999</td>\n",
              "      <td>488.869757</td>\n",
              "      <td>7265.463398</td>\n",
              "      <td>273.846325</td>\n",
              "      <td>43.333333</td>\n",
              "      <td>43.333333</td>\n",
              "      <td>43.333333</td>\n",
              "      <td>43.333333</td>\n",
              "      <td>247.736667</td>\n",
              "      <td>55.569024</td>\n",
              "      <td>7668.382560</td>\n",
              "      <td>295.900037</td>\n",
              "      <td>43.333333</td>\n",
              "      <td>43.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2526697</th>\n",
              "      <td>7441</td>\n",
              "      <td>A040165002</td>\n",
              "      <td>A040000165</td>\n",
              "      <td>1</td>\n",
              "      <td>2020-08-21 01:06:50</td>\n",
              "      <td>8836</td>\n",
              "      <td>train</td>\n",
              "      <td>4</td>\n",
              "      <td>165</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>21</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>46.0</td>\n",
              "      <td>46.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>6</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.652500</td>\n",
              "      <td>783</td>\n",
              "      <td>0.698551</td>\n",
              "      <td>2410</td>\n",
              "      <td>8266.00000</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>8837.122999</td>\n",
              "      <td>488.869757</td>\n",
              "      <td>7265.463398</td>\n",
              "      <td>273.846325</td>\n",
              "      <td>43.333333</td>\n",
              "      <td>43.333333</td>\n",
              "      <td>43.333333</td>\n",
              "      <td>43.333333</td>\n",
              "      <td>226.533333</td>\n",
              "      <td>27.315436</td>\n",
              "      <td>7668.382560</td>\n",
              "      <td>295.900037</td>\n",
              "      <td>43.333333</td>\n",
              "      <td>43.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2526698</th>\n",
              "      <td>7441</td>\n",
              "      <td>A040165003</td>\n",
              "      <td>A040000165</td>\n",
              "      <td>1</td>\n",
              "      <td>2020-08-21 01:07:36</td>\n",
              "      <td>8836</td>\n",
              "      <td>train</td>\n",
              "      <td>4</td>\n",
              "      <td>165</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>21</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>73.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>7</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.00</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.652500</td>\n",
              "      <td>783</td>\n",
              "      <td>0.698551</td>\n",
              "      <td>2410</td>\n",
              "      <td>8266.00000</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>8837.122999</td>\n",
              "      <td>488.869757</td>\n",
              "      <td>7265.463398</td>\n",
              "      <td>273.846325</td>\n",
              "      <td>43.333333</td>\n",
              "      <td>43.333333</td>\n",
              "      <td>43.333333</td>\n",
              "      <td>43.333333</td>\n",
              "      <td>228.453333</td>\n",
              "      <td>174.050336</td>\n",
              "      <td>7668.382560</td>\n",
              "      <td>295.900037</td>\n",
              "      <td>43.333333</td>\n",
              "      <td>43.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2526699</th>\n",
              "      <td>7441</td>\n",
              "      <td>A040165004</td>\n",
              "      <td>A040000165</td>\n",
              "      <td>1</td>\n",
              "      <td>2020-08-21 01:08:49</td>\n",
              "      <td>8836</td>\n",
              "      <td>train</td>\n",
              "      <td>4</td>\n",
              "      <td>165</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>21</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>8</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1.00</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.652500</td>\n",
              "      <td>783</td>\n",
              "      <td>0.698551</td>\n",
              "      <td>2410</td>\n",
              "      <td>8266.00000</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>8837.122999</td>\n",
              "      <td>488.869757</td>\n",
              "      <td>7265.463398</td>\n",
              "      <td>273.846325</td>\n",
              "      <td>43.333333</td>\n",
              "      <td>43.333333</td>\n",
              "      <td>43.333333</td>\n",
              "      <td>43.333333</td>\n",
              "      <td>35814.801394</td>\n",
              "      <td>28185.000000</td>\n",
              "      <td>7668.382560</td>\n",
              "      <td>295.900037</td>\n",
              "      <td>43.333333</td>\n",
              "      <td>43.333333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2526700 rows × 45 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         user assessmentItemID      testId  answer            Timestamp   tag  \\\n",
              "0           0       A060001001  A060000001       1  2020-03-24 00:17:11  7224   \n",
              "1           0       A060001002  A060000001       1  2020-03-24 00:17:14  7225   \n",
              "2           0       A060001003  A060000001       1  2020-03-24 00:17:22  7225   \n",
              "3           0       A060001004  A060000001       1  2020-03-24 00:17:29  7225   \n",
              "4           0       A060001005  A060000001       1  2020-03-24 00:17:36  7225   \n",
              "...       ...              ...         ...     ...                  ...   ...   \n",
              "2526695  7441       A030071005  A030000071       0  2020-06-05 06:50:21   438   \n",
              "2526696  7441       A040165001  A040000165       1  2020-08-21 01:06:39  8836   \n",
              "2526697  7441       A040165002  A040000165       1  2020-08-21 01:06:50  8836   \n",
              "2526698  7441       A040165003  A040000165       1  2020-08-21 01:07:36  8836   \n",
              "2526699  7441       A040165004  A040000165       1  2020-08-21 01:08:49  8836   \n",
              "\n",
              "         label  category  test  item  month  day  weekday  hour  elapsed  \\\n",
              "0        train         6     1     1      3   24        1     0      3.0   \n",
              "1        train         6     1     2      3   24        1     0      8.0   \n",
              "2        train         6     1     3      3   24        1     0      7.0   \n",
              "3        train         6     1     4      3   24        1     0      7.0   \n",
              "4        train         6     1     5      3   24        1     0     11.0   \n",
              "...        ...       ...   ...   ...    ...  ...      ...   ...      ...   \n",
              "2526695  train         3    71     5      6    5        4     6  65778.0   \n",
              "2526696  train         4   165     1      8   21        4     1     11.0   \n",
              "2526697  train         4   165     2      8   21        4     1     46.0   \n",
              "2526698  train         4   165     3      8   21        4     1     73.0   \n",
              "2526699  train         4   165     4      8   21        4     1      NaN   \n",
              "\n",
              "         test_elapsed  user_correct_answer  user_total_answer  user_acc  \\\n",
              "0                 3.0                  NaN                  0       NaN   \n",
              "1                 8.0                  1.0                  1  1.000000   \n",
              "2                 7.0                  2.0                  2  1.000000   \n",
              "3                 7.0                  3.0                  3  1.000000   \n",
              "4                11.0                  4.0                  4  1.000000   \n",
              "...               ...                  ...                ...       ...   \n",
              "2526695           NaN                  1.0                  4  0.250000   \n",
              "2526696          11.0                  1.0                  5  0.200000   \n",
              "2526697          46.0                  2.0                  6  0.333333   \n",
              "2526698          73.0                  3.0                  7  0.428571   \n",
              "2526699           NaN                  4.0                  8  0.500000   \n",
              "\n",
              "         user_category_correct_answer  user_category_total_answer  \\\n",
              "0                                 NaN                           0   \n",
              "1                                 1.0                           1   \n",
              "2                                 2.0                           2   \n",
              "3                                 3.0                           3   \n",
              "4                                 4.0                           4   \n",
              "...                               ...                         ...   \n",
              "2526695                           1.0                           4   \n",
              "2526696                           NaN                           0   \n",
              "2526697                           1.0                           1   \n",
              "2526698                           2.0                           2   \n",
              "2526699                           3.0                           3   \n",
              "\n",
              "         user_category_acc  user_tag_correct_answer  user_tag_total_answer  \\\n",
              "0                      NaN                      NaN                      0   \n",
              "1                     1.00                      NaN                      0   \n",
              "2                     1.00                      1.0                      1   \n",
              "3                     1.00                      2.0                      2   \n",
              "4                     1.00                      3.0                      3   \n",
              "...                    ...                      ...                    ...   \n",
              "2526695               0.25                      1.0                      4   \n",
              "2526696                NaN                      NaN                      0   \n",
              "2526697               1.00                      1.0                      1   \n",
              "2526698               1.00                      2.0                      2   \n",
              "2526699               1.00                      3.0                      3   \n",
              "\n",
              "         user_tag_acc  test_mean  test_sum  tag_mean  tag_sum  elapsed_user  \\\n",
              "0                 NaN   0.952667      1429  0.957333      718    6171.22043   \n",
              "1                 NaN   0.952667      1429  0.917067     3439    6171.22043   \n",
              "2                1.00   0.952667      1429  0.917067     3439    6171.22043   \n",
              "3                1.00   0.952667      1429  0.917067     3439    6171.22043   \n",
              "4                1.00   0.952667      1429  0.917067     3439    6171.22043   \n",
              "...               ...        ...       ...       ...      ...           ...   \n",
              "2526695          0.25   0.666000       999  0.694889     3127    8266.00000   \n",
              "2526696           NaN   0.652500       783  0.698551     2410    8266.00000   \n",
              "2526697          1.00   0.652500       783  0.698551     2410    8266.00000   \n",
              "2526698          1.00   0.652500       783  0.698551     2410    8266.00000   \n",
              "2526699          1.00   0.652500       783  0.698551     2410    8266.00000   \n",
              "\n",
              "         test_elapsed_user  elapsed_testId  test_elapsed_testId  \\\n",
              "0                50.592824     7022.485333            41.259585   \n",
              "1                50.592824     7022.485333            41.259585   \n",
              "2                50.592824     7022.485333            41.259585   \n",
              "3                50.592824     7022.485333            41.259585   \n",
              "4                50.592824     7022.485333            41.259585   \n",
              "...                    ...             ...                  ...   \n",
              "2526695          50.000000     7818.508339           388.870539   \n",
              "2526696          50.000000     8837.122999           488.869757   \n",
              "2526697          50.000000     8837.122999           488.869757   \n",
              "2526698          50.000000     8837.122999           488.869757   \n",
              "2526699          50.000000     8837.122999           488.869757   \n",
              "\n",
              "         elapsed_category  test_elapsed_category  elapsed_user_testId  \\\n",
              "0             5951.348344             231.106563          3348.666667   \n",
              "1             5951.348344             231.106563          3348.666667   \n",
              "2             5951.348344             231.106563          3348.666667   \n",
              "3             5951.348344             231.106563          3348.666667   \n",
              "4             5951.348344             231.106563          3348.666667   \n",
              "...                   ...                    ...                  ...   \n",
              "2526695       7109.351951             284.402511         13199.600000   \n",
              "2526696       7265.463398             273.846325            43.333333   \n",
              "2526697       7265.463398             273.846325            43.333333   \n",
              "2526698       7265.463398             273.846325            43.333333   \n",
              "2526699       7265.463398             273.846325            43.333333   \n",
              "\n",
              "         test_elapsed_user_testId  elapsed_user_category  \\\n",
              "0                        7.200000            7150.878613   \n",
              "1                        7.200000            7150.878613   \n",
              "2                        7.200000            7150.878613   \n",
              "3                        7.200000            7150.878613   \n",
              "4                        7.200000            7150.878613   \n",
              "...                           ...                    ...   \n",
              "2526695                 55.000000           13199.600000   \n",
              "2526696                 43.333333              43.333333   \n",
              "2526697                 43.333333              43.333333   \n",
              "2526698                 43.333333              43.333333   \n",
              "2526699                 43.333333              43.333333   \n",
              "\n",
              "         test_elapsed_user_category  elapsed_assessmentItemID  \\\n",
              "0                         44.469595                 13.660000   \n",
              "1                         44.469595                 26.112000   \n",
              "2                         44.469595                 19.180000   \n",
              "3                         44.469595                 18.076000   \n",
              "4                         44.469595                 35.720000   \n",
              "...                             ...                       ...   \n",
              "2526695                   55.000000              38109.581940   \n",
              "2526696                   43.333333                247.736667   \n",
              "2526697                   43.333333                226.533333   \n",
              "2526698                   43.333333                228.453333   \n",
              "2526699                   43.333333              35814.801394   \n",
              "\n",
              "         test_elapsed_assessmentItemID  elapsed_tag  test_elapsed_tag  \\\n",
              "0                            13.660000   108.744000         19.895861   \n",
              "1                            26.112000  7416.078667        113.686398   \n",
              "2                            19.180000  7416.078667        113.686398   \n",
              "3                            18.076000  7416.078667        113.686398   \n",
              "4                            35.720000  7416.078667        113.686398   \n",
              "...                                ...          ...               ...   \n",
              "2526695                   33704.500000  7430.302067        237.235946   \n",
              "2526696                      55.569024  7668.382560        295.900037   \n",
              "2526697                      27.315436  7668.382560        295.900037   \n",
              "2526698                     174.050336  7668.382560        295.900037   \n",
              "2526699                   28185.000000  7668.382560        295.900037   \n",
              "\n",
              "         elapsed_user_tag  test_elapsed_user_tag  \n",
              "0                3.000000               3.000000  \n",
              "1             4017.800000               8.250000  \n",
              "2             4017.800000               8.250000  \n",
              "3             4017.800000               8.250000  \n",
              "4             4017.800000               8.250000  \n",
              "...                   ...                    ...  \n",
              "2526695      13199.600000              55.000000  \n",
              "2526696         43.333333              43.333333  \n",
              "2526697         43.333333              43.333333  \n",
              "2526698         43.333333              43.333333  \n",
              "2526699         43.333333              43.333333  \n",
              "\n",
              "[2526700 rows x 45 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VZzei3DhfQy"
      },
      "source": [
        "## 3. Train/Test 데이터 셋 분리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(730, 1860), (481, 1847), (1112, 1777), (394, 1774), (926, 1773)]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(zip(df['user'].value_counts().index, df['user'].value_counts()))[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-05-24T09:49:29.684739Z",
          "start_time": "2021-05-24T09:49:28.982Z"
        },
        "id": "YOPWK7ckhfQz"
      },
      "outputs": [],
      "source": [
        "# train과 test 데이터셋은 사용자 별로 묶어서 분리를 해주어야함\n",
        "random.seed(42)\n",
        "def custom_train_test_split(df, ratio=0.7, split=True):\n",
        "    \n",
        "    users = list(zip(df['userID'].value_counts().index, df['userID'].value_counts()))\n",
        "    random.shuffle(users)\n",
        "    \n",
        "    max_train_data_len = ratio*len(df)\n",
        "    sum_of_train_data = 0\n",
        "    user_ids =[]\n",
        "\n",
        "    for user_id, count in users:\n",
        "        sum_of_train_data += count\n",
        "        if max_train_data_len < sum_of_train_data:\n",
        "            break\n",
        "        user_ids.append(user_id)\n",
        "\n",
        "\n",
        "    train = df[df['userID'].isin(user_ids)]\n",
        "    test = df[df['userID'].isin(user_ids) == False]\n",
        "\n",
        "    #test데이터셋은 각 유저의 마지막 interaction만 추출\n",
        "    test = test[test['userID'] != test['userID'].shift(-1)]\n",
        "    return train, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "random.seed(42)\n",
        "def train_test_split(df):\n",
        "\n",
        "    train = df[df[\"label\"] == \"train\"]\n",
        "    valid = df[df[\"label\"] == \"test\"]\n",
        "    test = valid[valid[\"answer\"] == -1]\n",
        "    valid = valid[valid[\"answer\"] != -1]\n",
        "    valid = valid[valid['user'] != valid['user'].shift(-1)]\n",
        "\n",
        "    return train, valid, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['user', 'assessmentItemID', 'testId', 'answer', 'Timestamp', 'tag',\n",
              "       'label', 'category', 'test', 'item', 'month', 'day', 'weekday', 'hour',\n",
              "       'elapsed', 'test_elapsed', 'user_correct_answer', 'user_total_answer',\n",
              "       'user_acc', 'user_category_correct_answer',\n",
              "       'user_category_total_answer', 'user_category_acc',\n",
              "       'user_tag_correct_answer', 'user_tag_total_answer', 'user_tag_acc',\n",
              "       'test_mean', 'test_sum', 'tag_mean', 'tag_sum', 'elapsed_user',\n",
              "       'test_elapsed_user', 'elapsed_testId', 'test_elapsed_testId',\n",
              "       'elapsed_category', 'test_elapsed_category', 'elapsed_user_testId',\n",
              "       'test_elapsed_user_testId', 'elapsed_user_category',\n",
              "       'test_elapsed_user_category', 'elapsed_assessmentItemID',\n",
              "       'test_elapsed_assessmentItemID', 'elapsed_tag', 'test_elapsed_tag',\n",
              "       'elapsed_user_tag', 'test_elapsed_user_tag'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-05-24T09:49:29.686739Z",
          "start_time": "2021-05-24T09:49:28.984Z"
        },
        "id": "i3HzdoybhfQ0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "40 Index(['user', 'tag', 'category', 'test', 'item', 'month', 'day', 'weekday',\n",
            "       'hour', 'elapsed', 'test_elapsed', 'user_correct_answer',\n",
            "       'user_total_answer', 'user_acc', 'user_category_correct_answer',\n",
            "       'user_category_total_answer', 'user_category_acc',\n",
            "       'user_tag_correct_answer', 'user_tag_total_answer', 'user_tag_acc',\n",
            "       'test_mean', 'test_sum', 'tag_mean', 'tag_sum', 'elapsed_user',\n",
            "       'test_elapsed_user', 'elapsed_testId', 'test_elapsed_testId',\n",
            "       'elapsed_category', 'test_elapsed_category', 'elapsed_user_testId',\n",
            "       'test_elapsed_user_testId', 'elapsed_user_category',\n",
            "       'test_elapsed_user_category', 'elapsed_assessmentItemID',\n",
            "       'test_elapsed_assessmentItemID', 'elapsed_tag', 'test_elapsed_tag',\n",
            "       'elapsed_user_tag', 'test_elapsed_user_tag'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# 유저별 분리\n",
        "train, valid, test = train_test_split(df)\n",
        "\n",
        "# 사용할 Feature 설정\n",
        "FEATS = ['KnowledgeTag', 'user_correct_answer', 'user_total_answer', \n",
        "         'user_acc', 'test_mean', 'test_sum', 'tag_mean','tag_sum']\n",
        "FEATS = df.columns.drop(['answer', \"assessmentItemID\", \"testId\", \"Timestamp\", \"label\"])\n",
        "print(len(FEATS), FEATS)\n",
        "\n",
        "# X, y 값 분리\n",
        "y_train = train['answer']\n",
        "train = train.drop(['answer'], axis=1)\n",
        "\n",
        "y_valid = valid['answer']\n",
        "valid = valid.drop(['answer'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2988       0\n",
              "3659       1\n",
              "10859      1\n",
              "15277      1\n",
              "23530      0\n",
              "          ..\n",
              "2525937    0\n",
              "2526080    1\n",
              "2526281    0\n",
              "2526296    0\n",
              "2526674    1\n",
              "Name: answer, Length: 744, dtype: int64"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-05-24T09:49:29.687739Z",
          "start_time": "2021-05-24T09:49:28.985Z"
        },
        "id": "mwAF79FbhfQ1"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-05-24T09:49:29.688736Z",
          "start_time": "2021-05-24T09:49:28.986Z"
        },
        "id": "o59GhDdVhfQ1"
      },
      "outputs": [],
      "source": [
        "lgb_train = lgb.Dataset(train[FEATS], y_train)\n",
        "lgb_valid = lgb.Dataset(valid[FEATS], y_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiHss_BBhfQ2"
      },
      "source": [
        "## 4. 훈련 및 검증"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-05-24T09:49:29.689738Z",
          "start_time": "2021-05-24T09:49:28.988Z"
        },
        "id": "-6FZfjA_hfQ2"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "from wandb.lightgbm import wandb_callback\n",
        "wandb.init(project=\"LGBM\", entity=\"recsys_01\", name=\"fe_40\")\n",
        "\n",
        "model = lgb.train(\n",
        "    {'objective': 'binary',\n",
        "    'bagging_fraction': 0.7671,\n",
        "    'feature_fraction': 0.6808,\n",
        "    'learning_rate': 0.5731,\n",
        "    'max_depth': 11,\n",
        "    'min_child_samples': 272,\n",
        "    'min_data_in_leaf': 149,\n",
        "    'n_estimators': 266,\n",
        "    'num_iterations': 1230,\n",
        "    'num_leaves': 103\n",
        "    }, \n",
        "    lgb_train,\n",
        "    valid_sets=[lgb_train, lgb_valid],\n",
        "    verbose_eval=100,\n",
        "    num_boost_round=500,\n",
        "    early_stopping_rounds=55, \n",
        "    callbacks=[wandb_callback()]\n",
        ")\n",
        "\n",
        "preds = model.predict(valid[FEATS])\n",
        "acc = accuracy_score(y_valid, np.where(preds >= 0.5, 1, 0))\n",
        "auc = roc_auc_score(y_valid, preds)\n",
        "\n",
        "print(f'VALID AUC : {auc} ACC : {acc}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4-1. wandb sweep 활용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create sweep with ID: hhobp47x\n",
            "Sweep URL: https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\n"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "from wandb.lightgbm import wandb_callback\n",
        "\n",
        "sweep_config = {\n",
        "  \"name\" : \"[LGBM] sweep\",\n",
        "  \"method\" : \"bayes\",\n",
        "  \"metric\" : {\n",
        "      \"goal\" : \"minimize\",\n",
        "      \"name\" : \"val_loss\"\n",
        "  },\n",
        "  \"parameters\" : {\n",
        "    \"early_stopping_round\" :{\n",
        "      \"min\": 50,\n",
        "      \"max\": 200,\n",
        "      \"distribution\": \"int_uniform\"\n",
        "    },\n",
        "    \"learning_rate\" :{\n",
        "      \"min\": 0.3,\n",
        "      \"max\": 1,\n",
        "      \"distribution\": \"uniform\"\n",
        "    },\n",
        "    \"max_depth\" :{\n",
        "      \"min\": 5,\n",
        "      \"max\": 30,\n",
        "      \"distribution\": \"int_uniform\"\n",
        "    },\n",
        "    \"n_estimators\" :{\n",
        "      \"min\": 50,\n",
        "      \"max\": 350,\n",
        "      \"distribution\": \"int_uniform\"\n",
        "    },\n",
        "    \"min_child_samples\" :{\n",
        "      \"min\": 10,\n",
        "      \"max\": 400,\n",
        "      \"distribution\": \"int_uniform\"\n",
        "    },\n",
        "    \"num_iterations\" :{\n",
        "      \"min\": 500,\n",
        "      \"max\": 1500,\n",
        "      \"distribution\": \"int_uniform\"\n",
        "    },\n",
        "    \"num_leaves\" :{\n",
        "      \"min\": 50,\n",
        "      \"max\": 250,\n",
        "      \"distribution\": \"int_uniform\"\n",
        "    },\n",
        "    \"bagging_fraction\" :{\n",
        "      \"min\": 0.5,\n",
        "      \"max\": 1.0,\n",
        "      \"distribution\": \"uniform\"\n",
        "    },\n",
        "    \"feature_fraction\" :{\n",
        "      \"min\": 0.5,\n",
        "      \"max\": 1.0,\n",
        "      \"distribution\": \"uniform\"\n",
        "    },\n",
        "    # \"max_bin\" :{\n",
        "    #   \"min\": 10,\n",
        "    #   \"max\": 300,\n",
        "    #   \"distribution\": \"int_uniform\"\n",
        "    # },\n",
        "    \"min_data_in_leaf\" :{\n",
        "      \"min\": 10,\n",
        "      \"max\": 150,\n",
        "      \"distribution\": \"int_uniform\"\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"LGBM\", entity=\"recsys_01\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load data\n",
        "def train():\n",
        "\n",
        "  wandb.init(project=\"LGBM\", entity=\"recsys_01\")\n",
        "  config = wandb.config\n",
        "\n",
        "  # fit model on train\n",
        "  model = lgb.train(\n",
        "    {\n",
        "      'objective': \"binary\",\n",
        "      'learning_rate': config.learning_rate,\n",
        "      'max_depth': config.max_depth,\n",
        "      'n_estimators': config.n_estimators,\n",
        "      'min_child_samples': config.min_child_samples,\n",
        "      'num_iterations': config.num_iterations,\n",
        "      'num_leaves': config.num_leaves,\n",
        "      'bagging_fraction': config.bagging_fraction,\n",
        "      'feature_fraction': config.feature_fraction,\n",
        "      # 'max_bin': config.max_bin,\n",
        "      'min_data_in_leaf': config.min_data_in_leaf,\n",
        "      'feature_pre_filter': False\n",
        "    },\n",
        "    lgb_train,\n",
        "    valid_sets=[lgb_train, lgb_valid],\n",
        "    verbose_eval=100,\n",
        "    num_boost_round=500,\n",
        "    early_stopping_rounds=config.early_stopping_round,\n",
        "    callbacks=[wandb_callback()]\n",
        ")\n",
        "\n",
        "  # make predictions on valid\n",
        "  y_pred = model.predict(valid[FEATS])\n",
        "\n",
        "  # evaluate predictions\n",
        "  acc = accuracy_score(y_valid, np.where(y_pred >= 0.5, 1, 0))\n",
        "  auc = roc_auc_score(y_valid, y_pred)\n",
        "\n",
        "  print(f'VALID AUC : {auc} ACC : {acc}\\n')\n",
        "  wandb.log({\"auc\": auc, \"acc\": acc})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: soxtfgb2 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.8360787048117926\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 50\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.8850008639882398\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.7820590792837139\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 281\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 25\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 291\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 573\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 85\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrecsys_01\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_012855-soxtfgb2</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/soxtfgb2\" target=\"_blank\">devoted-sweep-1</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047438 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "Early stopping, best iteration is:\n",
            "[41]\ttraining's binary_logloss: 0.455356\tvalid_1's binary_logloss: 0.523008\n",
            "VALID AUC : 0.821730320475776 ACC : 0.7405913978494624\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>training_binary_logloss</td><td>█▆▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▅▄▅▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▂▁▁▂▁▂▁▁▁▂▁▂▂▂▂▃▃▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.74059</td></tr><tr><td>auc</td><td>0.82173</td></tr><tr><td>iteration</td><td>90</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">devoted-sweep-1</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/soxtfgb2\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/soxtfgb2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_012855-soxtfgb2/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: d5jg9k2o with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.8068538636147345\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 133\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.5144265121030704\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.853671119356876\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 29\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 136\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 121\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 124\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 1280\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 131\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_012916-d5jg9k2o</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/d5jg9k2o\" target=\"_blank\">exalted-sweep-2</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=121, min_child_samples=136 will be ignored. Current value: min_data_in_leaf=121\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=121, min_child_samples=136 will be ignored. Current value: min_data_in_leaf=121\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=121, min_child_samples=136 will be ignored. Current value: min_data_in_leaf=121\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030887 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 133 rounds\n",
            "[100]\ttraining's binary_logloss: 0.428558\tvalid_1's binary_logloss: 0.505912\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[124]\ttraining's binary_logloss: 0.422413\tvalid_1's binary_logloss: 0.514722\n",
            "VALID AUC : 0.8281456880975545 ACC : 0.7446236559139785\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▆▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▇▇▇▆▅▄▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.74462</td></tr><tr><td>auc</td><td>0.82815</td></tr><tr><td>iteration</td><td>123</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">exalted-sweep-2</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/d5jg9k2o\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/d5jg9k2o</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_012916-d5jg9k2o/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mc679ldx with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.5557125757800605\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 193\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.939502752757492\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.4080202437700232\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 373\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 123\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 294\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 1452\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 173\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_012937-mc679ldx</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/mc679ldx\" target=\"_blank\">laced-sweep-3</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=123, min_child_samples=373 will be ignored. Current value: min_data_in_leaf=123\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=123, min_child_samples=373 will be ignored. Current value: min_data_in_leaf=123\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=123, min_child_samples=373 will be ignored. Current value: min_data_in_leaf=123\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048669 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 193 rounds\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[100]\ttraining's binary_logloss: 0.457519\tvalid_1's binary_logloss: 0.51341\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[200]\ttraining's binary_logloss: 0.442341\tvalid_1's binary_logloss: 0.499154\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[294]\ttraining's binary_logloss: 0.433941\tvalid_1's binary_logloss: 0.493502\n",
            "VALID AUC : 0.8397474817670424 ACC : 0.7594086021505376\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▇▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▇▇▆▅▅▅▅▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.75941</td></tr><tr><td>auc</td><td>0.83975</td></tr><tr><td>iteration</td><td>293</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">laced-sweep-3</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/mc679ldx\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/mc679ldx</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_012937-mc679ldx/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8bnt0t45 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.6134096417236854\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.6098175745204535\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.9922758232145183\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 339\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 26\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 1414\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 68\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_013013-8bnt0t45</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/8bnt0t45\" target=\"_blank\">earnest-sweep-4</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=339 will be ignored. Current value: min_data_in_leaf=26\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=339 will be ignored. Current value: min_data_in_leaf=26\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=26, min_child_samples=339 will be ignored. Current value: min_data_in_leaf=26\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.228231 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 64 rounds\n",
            "Early stopping, best iteration is:\n",
            "[35]\ttraining's binary_logloss: 0.464095\tvalid_1's binary_logloss: 0.512334\n",
            "VALID AUC : 0.8274946416684344 ACC : 0.7513440860215054\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▇▇▆▆▅▄▄▃▂▂▁▁▁▁▂▂▂▁▂▂▂▁▁▂▁▁▂▂▂▂▂▂▂▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.75134</td></tr><tr><td>auc</td><td>0.82749</td></tr><tr><td>iteration</td><td>98</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">earnest-sweep-4</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/8bnt0t45\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/8bnt0t45</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_013013-8bnt0t45/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tjht5y6e with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.6168390645460979\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 197\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.8689439027882242\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.8979763832408263\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 88\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 127\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 63\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 962\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 227\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_013034-tjht5y6e</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/tjht5y6e\" target=\"_blank\">dauntless-sweep-5</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=127, min_child_samples=88 will be ignored. Current value: min_data_in_leaf=127\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=127, min_child_samples=88 will be ignored. Current value: min_data_in_leaf=127\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=127, min_child_samples=88 will be ignored. Current value: min_data_in_leaf=127\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057080 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Training until validation scores don't improve for 197 rounds\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[63]\ttraining's binary_logloss: 0.447107\tvalid_1's binary_logloss: 0.502022\n",
            "VALID AUC : 0.8365361403919446 ACC : 0.7526881720430108\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>training_binary_logloss</td><td>█▇▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▇▆▇▆▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▃▃▃▃▂▃▂▂▂▂▂▂▁▁▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.75269</td></tr><tr><td>auc</td><td>0.83654</td></tr><tr><td>iteration</td><td>62</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">dauntless-sweep-5</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/tjht5y6e\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/tjht5y6e</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_013034-tjht5y6e/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5ud6ki70 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.969647695515608\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 98\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.5840334054935792\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.7305906536843829\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 381\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 78\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 51\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 887\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 247\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_013055-5ud6ki70</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/5ud6ki70\" target=\"_blank\">gentle-sweep-6</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=78, min_child_samples=381 will be ignored. Current value: min_data_in_leaf=78\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=78, min_child_samples=381 will be ignored. Current value: min_data_in_leaf=78\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=78, min_child_samples=381 will be ignored. Current value: min_data_in_leaf=78\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029934 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 98 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[51]\ttraining's binary_logloss: 0.432721\tvalid_1's binary_logloss: 0.514979\n",
            "VALID AUC : 0.8250660190339641 ACC : 0.7473118279569892\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▇▆▅▄▄▄▄▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.74731</td></tr><tr><td>auc</td><td>0.82507</td></tr><tr><td>iteration</td><td>50</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">gentle-sweep-6</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/5ud6ki70\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/5ud6ki70</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_013055-5ud6ki70/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: f874wsiu with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.6942891415307153\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 184\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.8630734856032839\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.721161581209439\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 30\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 275\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 40\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 59\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 1060\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 197\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_013117-f874wsiu</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/f874wsiu\" target=\"_blank\">lively-sweep-7</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=275 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=275 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=40, min_child_samples=275 will be ignored. Current value: min_data_in_leaf=40\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058297 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 184 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[59]\ttraining's binary_logloss: 0.431628\tvalid_1's binary_logloss: 0.510922\n",
            "VALID AUC : 0.826148658039692 ACC : 0.7526881720430108\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▆▅▅▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▁▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.75269</td></tr><tr><td>auc</td><td>0.82615</td></tr><tr><td>iteration</td><td>58</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">lively-sweep-7</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/f874wsiu\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/f874wsiu</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_013117-f874wsiu/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: no8n7rkm with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.9675377829781148\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 133\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.8624999862102705\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.4427810511626834\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 13\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 126\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 98\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 94\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 841\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 78\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_013138-no8n7rkm</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/no8n7rkm\" target=\"_blank\">snowy-sweep-8</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=98, min_child_samples=126 will be ignored. Current value: min_data_in_leaf=98\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=98, min_child_samples=126 will be ignored. Current value: min_data_in_leaf=98\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=98, min_child_samples=126 will be ignored. Current value: min_data_in_leaf=98\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055766 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 133 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[94]\ttraining's binary_logloss: 0.446578\tvalid_1's binary_logloss: 0.499538\n",
            "VALID AUC : 0.8362581655120955 ACC : 0.7513440860215054\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▅▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▅▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.75134</td></tr><tr><td>auc</td><td>0.83626</td></tr><tr><td>iteration</td><td>93</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">snowy-sweep-8</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/no8n7rkm\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/no8n7rkm</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_013138-no8n7rkm/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: i8vdmrr7 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.933868054178848\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 50\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.5530877211733038\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.6183822404927894\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 13\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 285\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 77\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 567\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 170\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_013159-i8vdmrr7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/i8vdmrr7\" target=\"_blank\">jolly-sweep-9</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=77, min_child_samples=285 will be ignored. Current value: min_data_in_leaf=77\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=77, min_child_samples=285 will be ignored. Current value: min_data_in_leaf=77\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=77, min_child_samples=285 will be ignored. Current value: min_data_in_leaf=77\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029814 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "[100]\ttraining's binary_logloss: 0.425412\tvalid_1's binary_logloss: 0.506403\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[128]\ttraining's binary_logloss: 0.417152\tvalid_1's binary_logloss: 0.510341\n",
            "VALID AUC : 0.8278018770619519 ACC : 0.7567204301075269\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▇▆▆▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▆▅▅▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▁▁▁▁▁▁▁▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.75672</td></tr><tr><td>auc</td><td>0.8278</td></tr><tr><td>iteration</td><td>127</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">jolly-sweep-9</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/i8vdmrr7\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/i8vdmrr7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_013159-i8vdmrr7/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ek75ehbg with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.5638638178513298\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 179\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.5258150577932723\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.8765550087618623\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 28\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 304\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 114\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 214\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 1302\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 71\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_013220-ek75ehbg</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/ek75ehbg\" target=\"_blank\">rare-sweep-10</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=114, min_child_samples=304 will be ignored. Current value: min_data_in_leaf=114\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=114, min_child_samples=304 will be ignored. Current value: min_data_in_leaf=114\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=114, min_child_samples=304 will be ignored. Current value: min_data_in_leaf=114\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038930 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 179 rounds\n",
            "[100]\ttraining's binary_logloss: 0.443833\tvalid_1's binary_logloss: 0.524916\n",
            "[200]\ttraining's binary_logloss: 0.42678\tvalid_1's binary_logloss: 0.531171\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[214]\ttraining's binary_logloss: 0.424857\tvalid_1's binary_logloss: 0.532907\n",
            "VALID AUC : 0.8148540997637214 ACC : 0.7526881720430108\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: Network error (ReadTimeout), entering retry loop.\n",
            "wandb: Network error (ReadTimeout), entering retry loop.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▇▆▆▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▇▆▄▃▂▂▁▁▁▁▂▁▁▂▂▂▂▃▂▃▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.75269</td></tr><tr><td>auc</td><td>0.81485</td></tr><tr><td>iteration</td><td>213</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">rare-sweep-10</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/ek75ehbg\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/ek75ehbg</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_013220-ek75ehbg/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cu450l5f with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.98122758334022\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 191\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.562213570613332\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.3060859107943605\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 143\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 94\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 75\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 933\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 240\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_013404-cu450l5f</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/cu450l5f\" target=\"_blank\">vibrant-sweep-11</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=143 will be ignored. Current value: min_data_in_leaf=94\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=143 will be ignored. Current value: min_data_in_leaf=94\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=94, min_child_samples=143 will be ignored. Current value: min_data_in_leaf=94\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032299 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 191 rounds\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[75]\ttraining's binary_logloss: 0.475998\tvalid_1's binary_logloss: 0.537012\n",
            "VALID AUC : 0.804130121504283 ACC : 0.7258064516129032\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▆▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.72581</td></tr><tr><td>auc</td><td>0.80413</td></tr><tr><td>iteration</td><td>74</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">vibrant-sweep-11</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/cu450l5f\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/cu450l5f</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_013404-cu450l5f/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7kbofbvq with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.892298635756398\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 75\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.5909593488462194\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.4388759676935259\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 390\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 143\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 303\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 513\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 115\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_013432-7kbofbvq</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/7kbofbvq\" target=\"_blank\">wild-sweep-12</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=143, min_child_samples=390 will be ignored. Current value: min_data_in_leaf=143\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=143, min_child_samples=390 will be ignored. Current value: min_data_in_leaf=143\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=143, min_child_samples=390 will be ignored. Current value: min_data_in_leaf=143\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037527 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 75 rounds\n",
            "[100]\ttraining's binary_logloss: 0.43954\tvalid_1's binary_logloss: 0.494639\n",
            "[200]\ttraining's binary_logloss: 0.422196\tvalid_1's binary_logloss: 0.494586\n",
            "Early stopping, best iteration is:\n",
            "[127]\ttraining's binary_logloss: 0.433983\tvalid_1's binary_logloss: 0.491085\n",
            "VALID AUC : 0.839623124583952 ACC : 0.7701612903225806\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▅▅▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▆▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.77016</td></tr><tr><td>auc</td><td>0.83962</td></tr><tr><td>iteration</td><td>201</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">wild-sweep-12</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/7kbofbvq\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/7kbofbvq</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_013432-7kbofbvq/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gi38eepp with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.9362587241725474\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 150\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.7538804127413499\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.4225005455552864\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 326\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 141\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 294\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 695\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 142\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_013504-gi38eepp</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/gi38eepp\" target=\"_blank\">chocolate-sweep-13</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=141, min_child_samples=326 will be ignored. Current value: min_data_in_leaf=141\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=141, min_child_samples=326 will be ignored. Current value: min_data_in_leaf=141\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=141, min_child_samples=326 will be ignored. Current value: min_data_in_leaf=141\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050938 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 150 rounds\n",
            "[100]\ttraining's binary_logloss: 0.434413\tvalid_1's binary_logloss: 0.498162\n",
            "[200]\ttraining's binary_logloss: 0.415246\tvalid_1's binary_logloss: 0.499208\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[294]\ttraining's binary_logloss: 0.399527\tvalid_1's binary_logloss: 0.506271\n",
            "VALID AUC : 0.8313570294726524 ACC : 0.760752688172043\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▇▆▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▆▄▃▂▃▂▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.76075</td></tr><tr><td>auc</td><td>0.83136</td></tr><tr><td>iteration</td><td>293</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">chocolate-sweep-13</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/gi38eepp\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/gi38eepp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_013504-gi38eepp/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: w7mx34de with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.8068076323526259\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 62\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.785950808205536\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.3947657144248851\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 22\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 172\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 127\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 285\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 828\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 135\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_013602-w7mx34de</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/w7mx34de\" target=\"_blank\">hardy-sweep-14</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=127, min_child_samples=172 will be ignored. Current value: min_data_in_leaf=127\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=127, min_child_samples=172 will be ignored. Current value: min_data_in_leaf=127\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=127, min_child_samples=172 will be ignored. Current value: min_data_in_leaf=127\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.072461 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 62 rounds\n",
            "[100]\ttraining's binary_logloss: 0.436927\tvalid_1's binary_logloss: 0.500422\n",
            "Early stopping, best iteration is:\n",
            "[94]\ttraining's binary_logloss: 0.438142\tvalid_1's binary_logloss: 0.497841\n",
            "VALID AUC : 0.8375456281135015 ACC : 0.7486559139784946\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▅▅▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▅▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.74866</td></tr><tr><td>auc</td><td>0.83755</td></tr><tr><td>iteration</td><td>155</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">hardy-sweep-14</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/w7mx34de\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/w7mx34de</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_013602-w7mx34de/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: p2vyhgm7 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.8806480429641291\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 198\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.7551910664522063\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.6896218505429339\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 50\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 101\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 239\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 527\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 52\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_013644-p2vyhgm7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/p2vyhgm7\" target=\"_blank\">noble-sweep-15</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=101, min_child_samples=50 will be ignored. Current value: min_data_in_leaf=101\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=101, min_child_samples=50 will be ignored. Current value: min_data_in_leaf=101\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=101, min_child_samples=50 will be ignored. Current value: min_data_in_leaf=101\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.098214 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 198 rounds\n",
            "[100]\ttraining's binary_logloss: 0.4491\tvalid_1's binary_logloss: 0.505958\n",
            "[200]\ttraining's binary_logloss: 0.435319\tvalid_1's binary_logloss: 0.501134\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[239]\ttraining's binary_logloss: 0.430888\tvalid_1's binary_logloss: 0.504551\n",
            "VALID AUC : 0.8303402266226784 ACC : 0.7526881720430108\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▆▆▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.75269</td></tr><tr><td>auc</td><td>0.83034</td></tr><tr><td>iteration</td><td>238</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">noble-sweep-15</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/p2vyhgm7\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/p2vyhgm7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_013644-p2vyhgm7/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7vnbh7s2 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.8898349560977477\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 173\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.9864506326208002\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.5935521263030448\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 28\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 94\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 76\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 274\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 1298\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 212\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_013728-7vnbh7s2</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/7vnbh7s2\" target=\"_blank\">easy-sweep-16</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=76, min_child_samples=94 will be ignored. Current value: min_data_in_leaf=76\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=76, min_child_samples=94 will be ignored. Current value: min_data_in_leaf=76\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=76, min_child_samples=94 will be ignored. Current value: min_data_in_leaf=76\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092763 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 173 rounds\n",
            "[100]\ttraining's binary_logloss: 0.415789\tvalid_1's binary_logloss: 0.502841\n",
            "[200]\ttraining's binary_logloss: 0.384444\tvalid_1's binary_logloss: 0.539049\n",
            "Early stopping, best iteration is:\n",
            "[95]\ttraining's binary_logloss: 0.417439\tvalid_1's binary_logloss: 0.502398\n",
            "VALID AUC : 0.8326956979729778 ACC : 0.7580645161290323\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▇▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>▆▆▃▃▃▂▂▂▂▂▁▂▁▁▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▆▆▆▆▇▇▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.75806</td></tr><tr><td>auc</td><td>0.8327</td></tr><tr><td>iteration</td><td>267</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">easy-sweep-16</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/7vnbh7s2\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/7vnbh7s2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_013728-7vnbh7s2/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: a17uffzw with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.8126841819144859\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 96\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.6754001825255678\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.8359655214375814\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 304\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 22\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 206\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 967\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 157\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_013830-a17uffzw</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/a17uffzw\" target=\"_blank\">valiant-sweep-17</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=304 will be ignored. Current value: min_data_in_leaf=22\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=304 will be ignored. Current value: min_data_in_leaf=22\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=22, min_child_samples=304 will be ignored. Current value: min_data_in_leaf=22\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.517441 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 96 rounds\n",
            "[100]\ttraining's binary_logloss: 0.425877\tvalid_1's binary_logloss: 0.546233\n",
            "Early stopping, best iteration is:\n",
            "[45]\ttraining's binary_logloss: 0.445016\tvalid_1's binary_logloss: 0.509314\n",
            "VALID AUC : 0.8291917514611968 ACC : 0.7432795698924731\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▇▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>▆▆▄▄▁▁▁▁▂▂▂▂▁▁▂▂▂▃▂▃▄▄▄▅▆▇▇▇██▇█████▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.74328</td></tr><tr><td>auc</td><td>0.82919</td></tr><tr><td>iteration</td><td>140</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">valiant-sweep-17</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/a17uffzw\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/a17uffzw</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_013830-a17uffzw/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 904hn5jd with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.8792976382195856\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 59\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.7397438492174728\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.9413570963713052\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 45\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 50\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 267\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 1362\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 207\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_013925-904hn5jd</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/904hn5jd\" target=\"_blank\">soft-sweep-18</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=45 will be ignored. Current value: min_data_in_leaf=50\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=45 will be ignored. Current value: min_data_in_leaf=50\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=50, min_child_samples=45 will be ignored. Current value: min_data_in_leaf=50\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052707 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 59 rounds\n",
            "Early stopping, best iteration is:\n",
            "[21]\ttraining's binary_logloss: 0.453789\tvalid_1's binary_logloss: 0.517151\n",
            "VALID AUC : 0.8258158196967148 ACC : 0.7540322580645161\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▇▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▆▄▃▄▃▃▃▂▂▁▁▂▂▃▃▄▃▄▅▄▃▃▃▂▂▂▂▂▂▃▃▃▂▂▃▂▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.75403</td></tr><tr><td>auc</td><td>0.82582</td></tr><tr><td>iteration</td><td>79</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">soft-sweep-18</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/904hn5jd\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/904hn5jd</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_013925-904hn5jd/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: miu8bfnp with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.5101587610866513\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 118\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.944755868264514\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.6435831349727277\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 253\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 37\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 204\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 1134\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 239\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_013946-miu8bfnp</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/miu8bfnp\" target=\"_blank\">effortless-sweep-19</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=37, min_child_samples=253 will be ignored. Current value: min_data_in_leaf=37\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=37, min_child_samples=253 will be ignored. Current value: min_data_in_leaf=37\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=37, min_child_samples=253 will be ignored. Current value: min_data_in_leaf=37\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048872 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 118 rounds\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[100]\ttraining's binary_logloss: 0.450063\tvalid_1's binary_logloss: 0.511711\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[200]\ttraining's binary_logloss: 0.434779\tvalid_1's binary_logloss: 0.496328\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[204]\ttraining's binary_logloss: 0.434362\tvalid_1's binary_logloss: 0.496439\n",
            "VALID AUC : 0.8383868678814657 ACC : 0.7594086021505376\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▆▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▆▆▆▅▅▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.75941</td></tr><tr><td>auc</td><td>0.83839</td></tr><tr><td>iteration</td><td>203</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">effortless-sweep-19</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/miu8bfnp\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/miu8bfnp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_013946-miu8bfnp/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0hu1jth8 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.8623882146804578\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 51\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.9759068033683396\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.6552255177019289\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 370\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 127\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 223\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 679\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 128\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_014025-0hu1jth8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/0hu1jth8\" target=\"_blank\">swept-sweep-20</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=127, min_child_samples=370 will be ignored. Current value: min_data_in_leaf=127\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=127, min_child_samples=370 will be ignored. Current value: min_data_in_leaf=127\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=127, min_child_samples=370 will be ignored. Current value: min_data_in_leaf=127\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058278 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 51 rounds\n",
            "[100]\ttraining's binary_logloss: 0.430277\tvalid_1's binary_logloss: 0.502475\n",
            "Early stopping, best iteration is:\n",
            "[71]\ttraining's binary_logloss: 0.438387\tvalid_1's binary_logloss: 0.498208\n",
            "VALID AUC : 0.8372603381052354 ACC : 0.7567204301075269\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▅▅▅▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▅▅▄▄▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.75672</td></tr><tr><td>auc</td><td>0.83726</td></tr><tr><td>iteration</td><td>121</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">swept-sweep-20</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/0hu1jth8\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/0hu1jth8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_014025-0hu1jth8/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0ro0dk7w with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.9435308442747268\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 140\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.5803373142861336\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.8565547927301691\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 25\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 65\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 78\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 187\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 760\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 104\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_014057-0ro0dk7w</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/0ro0dk7w\" target=\"_blank\">still-sweep-21</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=78, min_child_samples=65 will be ignored. Current value: min_data_in_leaf=78\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=78, min_child_samples=65 will be ignored. Current value: min_data_in_leaf=78\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=78, min_child_samples=65 will be ignored. Current value: min_data_in_leaf=78\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038325 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 140 rounds\n",
            "[100]\ttraining's binary_logloss: 0.434631\tvalid_1's binary_logloss: 0.514592\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[187]\ttraining's binary_logloss: 0.416328\tvalid_1's binary_logloss: 0.518496\n",
            "VALID AUC : 0.8260901370123552 ACC : 0.7567204301075269\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▇▆▆▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▆▆▅▄▄▃▂▂▂▂▃▃▂▃▂▁▁▁▁▁▁▂▁▂▂▂▂▂▁▁▂▂▁▂▁▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.75672</td></tr><tr><td>auc</td><td>0.82609</td></tr><tr><td>iteration</td><td>186</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">still-sweep-21</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/0ro0dk7w\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/0ro0dk7w</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_014057-0ro0dk7w/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: slknv359 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.6885455444545532\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 81\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.6391036473272161\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.5624065412911832\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 134\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 35\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 180\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 1451\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 227\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_014124-slknv359</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/slknv359\" target=\"_blank\">toasty-sweep-22</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=134 will be ignored. Current value: min_data_in_leaf=35\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=134 will be ignored. Current value: min_data_in_leaf=35\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=35, min_child_samples=134 will be ignored. Current value: min_data_in_leaf=35\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052058 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 81 rounds\n",
            "[100]\ttraining's binary_logloss: 0.417868\tvalid_1's binary_logloss: 0.507712\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[180]\ttraining's binary_logloss: 0.391565\tvalid_1's binary_logloss: 0.511895\n",
            "VALID AUC : 0.826719238056224 ACC : 0.7620967741935484\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▆▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▅▅▄▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▂▂▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.7621</td></tr><tr><td>auc</td><td>0.82672</td></tr><tr><td>iteration</td><td>179</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">toasty-sweep-22</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/slknv359\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/slknv359</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_014124-slknv359/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x772a5zd with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.7407023059312303\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 164\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.691977162757077\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.6630843269449463\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 85\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 58\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 193\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 645\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 203\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_014155-x772a5zd</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/x772a5zd\" target=\"_blank\">prime-sweep-23</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=58, min_child_samples=85 will be ignored. Current value: min_data_in_leaf=58\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=58, min_child_samples=85 will be ignored. Current value: min_data_in_leaf=58\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=58, min_child_samples=85 will be ignored. Current value: min_data_in_leaf=58\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.356869 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 164 rounds\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[100]\ttraining's binary_logloss: 0.419392\tvalid_1's binary_logloss: 0.516024\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[193]\ttraining's binary_logloss: 0.391567\tvalid_1's binary_logloss: 0.542581\n",
            "VALID AUC : 0.8132155109982956 ACC : 0.7553763440860215\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▇▆▆▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▆▅▄▃▂▂▁▁▂▁▁▁▁▂▂▃▂▃▃▃▃▄▄▄▄▄▅▅▆▆▅▅▆▆▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.75538</td></tr><tr><td>auc</td><td>0.81322</td></tr><tr><td>iteration</td><td>192</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">prime-sweep-23</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/x772a5zd\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/x772a5zd</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_014155-x772a5zd/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7cqhb2y3 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.6346279891886986\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 148\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.9936419908873492\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.7708690210347069\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 120\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 71\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 320\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 1291\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 236\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_014248-7cqhb2y3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/7cqhb2y3\" target=\"_blank\">ancient-sweep-24</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=71, min_child_samples=120 will be ignored. Current value: min_data_in_leaf=71\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=71, min_child_samples=120 will be ignored. Current value: min_data_in_leaf=71\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=71, min_child_samples=120 will be ignored. Current value: min_data_in_leaf=71\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.089204 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 148 rounds\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[100]\ttraining's binary_logloss: 0.46038\tvalid_1's binary_logloss: 0.52132\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[200]\ttraining's binary_logloss: 0.447487\tvalid_1's binary_logloss: 0.511835\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[300]\ttraining's binary_logloss: 0.439139\tvalid_1's binary_logloss: 0.511887\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[320]\ttraining's binary_logloss: 0.437916\tvalid_1's binary_logloss: 0.512074\n",
            "VALID AUC : 0.8277506711630322 ACC : 0.7513440860215054\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▇▆▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▇▆▅▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.75134</td></tr><tr><td>auc</td><td>0.82775</td></tr><tr><td>iteration</td><td>319</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">ancient-sweep-24</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/7cqhb2y3\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/7cqhb2y3</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_014248-7cqhb2y3/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pll4r2nh with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.5487563967299249\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 192\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.9941232330894794\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.7528013757198785\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 295\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 67\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 80\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 820\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 169\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_014345-pll4r2nh</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/pll4r2nh\" target=\"_blank\">dry-sweep-25</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=67, min_child_samples=295 will be ignored. Current value: min_data_in_leaf=67\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=67, min_child_samples=295 will be ignored. Current value: min_data_in_leaf=67\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=67, min_child_samples=295 will be ignored. Current value: min_data_in_leaf=67\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.092949 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 192 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[80]\ttraining's binary_logloss: 0.426618\tvalid_1's binary_logloss: 0.514512\n",
            "VALID AUC : 0.8279847552723788 ACC : 0.7446236559139785\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▅▅▅▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▂▂▂▂▂▂▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.74462</td></tr><tr><td>auc</td><td>0.82798</td></tr><tr><td>iteration</td><td>79</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">dry-sweep-25</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/pll4r2nh\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/pll4r2nh</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_014345-pll4r2nh/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9ja4em1q with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.6297424506186327\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 50\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.7583126101525901\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.8484973327819161\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 242\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 58\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 305\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 621\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 75\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_014419-9ja4em1q</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/9ja4em1q\" target=\"_blank\">comic-sweep-26</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=58, min_child_samples=242 will be ignored. Current value: min_data_in_leaf=58\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=58, min_child_samples=242 will be ignored. Current value: min_data_in_leaf=58\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=58, min_child_samples=242 will be ignored. Current value: min_data_in_leaf=58\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061382 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 50 rounds\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[100]\ttraining's binary_logloss: 0.443547\tvalid_1's binary_logloss: 0.510893\n",
            "Early stopping, best iteration is:\n",
            "[75]\ttraining's binary_logloss: 0.449068\tvalid_1's binary_logloss: 0.508136\n",
            "VALID AUC : 0.8278311375756201 ACC : 0.7526881720430108\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▇▆▆▆▅▄▃▃▂▂▂▂▂▂▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▂▁▂▂▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.75269</td></tr><tr><td>auc</td><td>0.82783</td></tr><tr><td>iteration</td><td>124</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">comic-sweep-26</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/9ja4em1q\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/9ja4em1q</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_014419-9ja4em1q/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kmexp2be with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.5450391556205696\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 138\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.5229253419034452\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.6022160313039306\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 183\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 133\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 189\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 920\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 129\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_014441-kmexp2be</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/kmexp2be\" target=\"_blank\">quiet-sweep-27</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=133, min_child_samples=183 will be ignored. Current value: min_data_in_leaf=133\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=133, min_child_samples=183 will be ignored. Current value: min_data_in_leaf=133\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=133, min_child_samples=183 will be ignored. Current value: min_data_in_leaf=133\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029445 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 138 rounds\n",
            "[100]\ttraining's binary_logloss: 0.433184\tvalid_1's binary_logloss: 0.494772\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[189]\ttraining's binary_logloss: 0.414656\tvalid_1's binary_logloss: 0.513128\n",
            "VALID AUC : 0.829030818636021 ACC : 0.7419354838709677\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▆▅▄▄▃▃▂▂▂▂▂▁▁▁▁▂▁▂▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.74194</td></tr><tr><td>auc</td><td>0.82903</td></tr><tr><td>iteration</td><td>188</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">quiet-sweep-27</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/kmexp2be\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/kmexp2be</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_014441-kmexp2be/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: y44iq8ma with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.6098467530024048\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 100\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.549318689064475\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.73739870812797\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 110\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 180\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 534\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 81\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_014508-y44iq8ma</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/y44iq8ma\" target=\"_blank\">likely-sweep-28</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=110, min_child_samples=16 will be ignored. Current value: min_data_in_leaf=110\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=110, min_child_samples=16 will be ignored. Current value: min_data_in_leaf=110\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=110, min_child_samples=16 will be ignored. Current value: min_data_in_leaf=110\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037705 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\ttraining's binary_logloss: 0.441402\tvalid_1's binary_logloss: 0.50481\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[180]\ttraining's binary_logloss: 0.426951\tvalid_1's binary_logloss: 0.50002\n",
            "VALID AUC : 0.834451328793077 ACC : 0.7432795698924731\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▆▆▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▆▅▆▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.74328</td></tr><tr><td>auc</td><td>0.83445</td></tr><tr><td>iteration</td><td>179</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">likely-sweep-28</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/y44iq8ma\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/y44iq8ma</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_014508-y44iq8ma/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 559yjpv7 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.5185149392953992\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 70\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.8102663636925178\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.6325918779045594\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 337\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 121\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 206\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 921\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 247\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_014543-559yjpv7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/559yjpv7\" target=\"_blank\">comic-sweep-29</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=121, min_child_samples=337 will be ignored. Current value: min_data_in_leaf=121\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=121, min_child_samples=337 will be ignored. Current value: min_data_in_leaf=121\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=121, min_child_samples=337 will be ignored. Current value: min_data_in_leaf=121\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.077629 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 70 rounds\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[100]\ttraining's binary_logloss: 0.426479\tvalid_1's binary_logloss: 0.519054\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Early stopping, best iteration is:\n",
            "[44]\ttraining's binary_logloss: 0.445368\tvalid_1's binary_logloss: 0.503458\n",
            "VALID AUC : 0.832337256680541 ACC : 0.7567204301075269\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▅▅▅▅▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.75672</td></tr><tr><td>auc</td><td>0.83234</td></tr><tr><td>iteration</td><td>113</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">comic-sweep-29</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/559yjpv7\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/559yjpv7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_014543-559yjpv7/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gbjtbaye with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.8721498280609232\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 73\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.917664461637933\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.5392489747178257\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 24\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 46\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 139\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 173\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 1111\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 233\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_014618-gbjtbaye</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/gbjtbaye\" target=\"_blank\">valiant-sweep-30</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=46 will be ignored. Current value: min_data_in_leaf=139\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=46 will be ignored. Current value: min_data_in_leaf=139\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=139, min_child_samples=46 will be ignored. Current value: min_data_in_leaf=139\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.099379 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 73 rounds\n",
            "[100]\ttraining's binary_logloss: 0.415676\tvalid_1's binary_logloss: 0.507383\n",
            "Early stopping, best iteration is:\n",
            "[50]\ttraining's binary_logloss: 0.435053\tvalid_1's binary_logloss: 0.49583\n",
            "VALID AUC : 0.8371798716926475 ACC : 0.7634408602150538\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▅▅▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▄▃▃▃▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.76344</td></tr><tr><td>auc</td><td>0.83718</td></tr><tr><td>iteration</td><td>122</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">valiant-sweep-30</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/gbjtbaye\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/gbjtbaye</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_014618-gbjtbaye/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qvqhkw1h with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.7281610878368394\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 181\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.7712674539505067\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.753120010842451\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 23\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 295\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 74\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 106\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 560\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 223\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_014654-qvqhkw1h</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/qvqhkw1h\" target=\"_blank\">light-sweep-31</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=295 will be ignored. Current value: min_data_in_leaf=74\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=295 will be ignored. Current value: min_data_in_leaf=74\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=74, min_child_samples=295 will be ignored. Current value: min_data_in_leaf=74\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.340169 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 181 rounds\n",
            "[100]\ttraining's binary_logloss: 0.411769\tvalid_1's binary_logloss: 0.52934\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[106]\ttraining's binary_logloss: 0.409304\tvalid_1's binary_logloss: 0.532458\n",
            "VALID AUC : 0.8133764438234714 ACC : 0.7419354838709677\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▄▃▃▃▂▂▁▁▁▁▂▁▁▁▂▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▃▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.74194</td></tr><tr><td>auc</td><td>0.81338</td></tr><tr><td>iteration</td><td>105</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">light-sweep-31</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/qvqhkw1h\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/qvqhkw1h</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_014654-qvqhkw1h/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0b9igs8x with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.7968850837543792\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 114\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.7561498908018729\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.32127823699345914\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 337\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 134\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 189\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 1064\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 109\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_014735-0b9igs8x</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/0b9igs8x\" target=\"_blank\">feasible-sweep-32</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=337 will be ignored. Current value: min_data_in_leaf=134\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=337 will be ignored. Current value: min_data_in_leaf=134\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=134, min_child_samples=337 will be ignored. Current value: min_data_in_leaf=134\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070622 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 114 rounds\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[100]\ttraining's binary_logloss: 0.462075\tvalid_1's binary_logloss: 0.517844\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[189]\ttraining's binary_logloss: 0.449082\tvalid_1's binary_logloss: 0.509046\n",
            "VALID AUC : 0.8264485783047921 ACC : 0.7486559139784946\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▅▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.74866</td></tr><tr><td>auc</td><td>0.82645</td></tr><tr><td>iteration</td><td>188</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">feasible-sweep-32</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/0b9igs8x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/0b9igs8x</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_014735-0b9igs8x/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fsuc6415 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.8991960345311734\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 175\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.6046637887247919\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.698579120497299\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 13\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 302\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 71\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 213\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 1152\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 196\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_014824-fsuc6415</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/fsuc6415\" target=\"_blank\">floral-sweep-33</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=71, min_child_samples=302 will be ignored. Current value: min_data_in_leaf=71\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=71, min_child_samples=302 will be ignored. Current value: min_data_in_leaf=71\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=71, min_child_samples=302 will be ignored. Current value: min_data_in_leaf=71\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.343220 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 175 rounds\n",
            "[100]\ttraining's binary_logloss: 0.419035\tvalid_1's binary_logloss: 0.525215\n",
            "[200]\ttraining's binary_logloss: 0.387845\tvalid_1's binary_logloss: 0.564032\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[213]\ttraining's binary_logloss: 0.384314\tvalid_1's binary_logloss: 0.567073\n",
            "VALID AUC : 0.7964784971800181 ACC : 0.7379032258064516\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▇▆▆▅▅▅▅▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>▆▃▃▂▂▂▁▁▁▁▁▁▂▁▂▂▃▃▃▃▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.7379</td></tr><tr><td>auc</td><td>0.79648</td></tr><tr><td>iteration</td><td>212</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">floral-sweep-33</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/fsuc6415\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/fsuc6415</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_014824-fsuc6415/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 89fsjp1f with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.5707020628352694\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 110\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.8927021348214922\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.7661085777167329\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 19\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 299\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 101\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 151\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 1154\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 223\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_014918-89fsjp1f</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/89fsjp1f\" target=\"_blank\">restful-sweep-34</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=101, min_child_samples=299 will be ignored. Current value: min_data_in_leaf=101\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=101, min_child_samples=299 will be ignored. Current value: min_data_in_leaf=101\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=101, min_child_samples=299 will be ignored. Current value: min_data_in_leaf=101\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067780 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 110 rounds\n",
            "[100]\ttraining's binary_logloss: 0.409511\tvalid_1's binary_logloss: 0.530887\n",
            "Early stopping, best iteration is:\n",
            "[28]\ttraining's binary_logloss: 0.446146\tvalid_1's binary_logloss: 0.506286\n",
            "VALID AUC : 0.8303987476500149 ACC : 0.7526881720430108\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▇▆▆▆▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>▆▄▃▃▂▂▁▁▁▂▂▂▁▂▃▃▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▅▆▆▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.75269</td></tr><tr><td>auc</td><td>0.8304</td></tr><tr><td>iteration</td><td>137</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">restful-sweep-34</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/89fsjp1f\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/89fsjp1f</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_014918-89fsjp1f/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wagimrrp with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.7026265297706021\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 185\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.5891101441469447\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.5915807309238179\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 80\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 57\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 167\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 587\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 143\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_014944-wagimrrp</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/wagimrrp\" target=\"_blank\">rare-sweep-35</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=57, min_child_samples=80 will be ignored. Current value: min_data_in_leaf=57\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=57, min_child_samples=80 will be ignored. Current value: min_data_in_leaf=57\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=57, min_child_samples=80 will be ignored. Current value: min_data_in_leaf=57\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041038 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 185 rounds\n",
            "[100]\ttraining's binary_logloss: 0.429805\tvalid_1's binary_logloss: 0.509466\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[167]\ttraining's binary_logloss: 0.413945\tvalid_1's binary_logloss: 0.5126\n",
            "VALID AUC : 0.8261486580396918 ACC : 0.75\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▅▄▄▄▄▃▂▂▂▂▁▁▁▁▁▁▂▂▂▁▁▁▁▂▁▁▁▂▂▂▂▂▂▂▁▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.75</td></tr><tr><td>auc</td><td>0.82615</td></tr><tr><td>iteration</td><td>166</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">rare-sweep-35</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/wagimrrp\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/wagimrrp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_014944-wagimrrp/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gnd57olo with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.7049634092536946\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 157\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.6243871454481991\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.45722697474963114\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 384\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 73\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 350\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 1471\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 135\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_015013-gnd57olo</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/gnd57olo\" target=\"_blank\">autumn-sweep-36</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=73, min_child_samples=384 will be ignored. Current value: min_data_in_leaf=73\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=73, min_child_samples=384 will be ignored. Current value: min_data_in_leaf=73\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=73, min_child_samples=384 will be ignored. Current value: min_data_in_leaf=73\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041357 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 157 rounds\n",
            "[100]\ttraining's binary_logloss: 0.434735\tvalid_1's binary_logloss: 0.501033\n",
            "[200]\ttraining's binary_logloss: 0.41573\tvalid_1's binary_logloss: 0.509372\n",
            "Early stopping, best iteration is:\n",
            "[77]\ttraining's binary_logloss: 0.439671\tvalid_1's binary_logloss: 0.499227\n",
            "VALID AUC : 0.8361484385858393 ACC : 0.7553763440860215\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▅▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.75538</td></tr><tr><td>auc</td><td>0.83615</td></tr><tr><td>iteration</td><td>233</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">autumn-sweep-36</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/gnd57olo\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/gnd57olo</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_015013-gnd57olo/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: umw4xp38 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.6917536633449339\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 86\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.908809029686808\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.45418252774952306\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 132\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 73\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 157\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 1383\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 178\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_015045-umw4xp38</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/umw4xp38\" target=\"_blank\">helpful-sweep-37</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=73, min_child_samples=132 will be ignored. Current value: min_data_in_leaf=73\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=73, min_child_samples=132 will be ignored. Current value: min_data_in_leaf=73\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=73, min_child_samples=132 will be ignored. Current value: min_data_in_leaf=73\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052065 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 86 rounds\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[100]\ttraining's binary_logloss: 0.428552\tvalid_1's binary_logloss: 0.500138\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[157]\ttraining's binary_logloss: 0.414709\tvalid_1's binary_logloss: 0.499102\n",
            "VALID AUC : 0.834626891875087 ACC : 0.760752688172043\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▆▅▅▅▅▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.76075</td></tr><tr><td>auc</td><td>0.83463</td></tr><tr><td>iteration</td><td>156</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">helpful-sweep-37</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/umw4xp38\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/umw4xp38</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_015045-umw4xp38/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 89y22udf with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.7071168919347812\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 90\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.6973384706555078\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.6123691655303033\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 29\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 206\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 97\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 173\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 1120\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 117\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_015111-89y22udf</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/89y22udf\" target=\"_blank\">pleasant-sweep-38</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=97, min_child_samples=206 will be ignored. Current value: min_data_in_leaf=97\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=97, min_child_samples=206 will be ignored. Current value: min_data_in_leaf=97\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=97, min_child_samples=206 will be ignored. Current value: min_data_in_leaf=97\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055616 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 90 rounds\n",
            "[100]\ttraining's binary_logloss: 0.434021\tvalid_1's binary_logloss: 0.504606\n",
            "Early stopping, best iteration is:\n",
            "[69]\ttraining's binary_logloss: 0.441191\tvalid_1's binary_logloss: 0.500727\n",
            "VALID AUC : 0.8360313965311661 ACC : 0.7567204301075269\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▆▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▅▄▄▃▃▂▂▂▁▁▁▂▁▁▁▁▁▁▂▂▂▂▂▁▁▂▁▁▁▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.75672</td></tr><tr><td>auc</td><td>0.83603</td></tr><tr><td>iteration</td><td>158</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">pleasant-sweep-38</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/89y22udf\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/89y22udf</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_015111-89y22udf/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cejx5ka7 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.6156967545473873\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 102\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.5512211346242026\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.3865197748330575\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 117\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 144\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 60\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 718\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 189\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_015138-cejx5ka7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/cejx5ka7\" target=\"_blank\">cerulean-sweep-39</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=144, min_child_samples=117 will be ignored. Current value: min_data_in_leaf=144\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=144, min_child_samples=117 will be ignored. Current value: min_data_in_leaf=144\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=144, min_child_samples=117 will be ignored. Current value: min_data_in_leaf=144\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030740 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 102 rounds\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[60]\ttraining's binary_logloss: 0.476007\tvalid_1's binary_logloss: 0.524359\n",
            "VALID AUC : 0.8150369779741482 ACC : 0.7271505376344086\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▆▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.72715</td></tr><tr><td>auc</td><td>0.81504</td></tr><tr><td>iteration</td><td>59</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">cerulean-sweep-39</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/cejx5ka7\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/cejx5ka7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_015138-cejx5ka7/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: c4s6d617 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.973521799604416\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 76\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.6787068735552344\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.8810345022571611\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 279\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 121\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 95\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 674\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 117\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_015153-c4s6d617</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/c4s6d617\" target=\"_blank\">brisk-sweep-40</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=121, min_child_samples=279 will be ignored. Current value: min_data_in_leaf=121\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=121, min_child_samples=279 will be ignored. Current value: min_data_in_leaf=121\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=121, min_child_samples=279 will be ignored. Current value: min_data_in_leaf=121\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043340 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 76 rounds\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[95]\ttraining's binary_logloss: 0.434611\tvalid_1's binary_logloss: 0.521582\n",
            "VALID AUC : 0.8207354630110532 ACC : 0.7580645161290323\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▆▆▆▆▅▅▄▃▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂▂▂▂▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.75806</td></tr><tr><td>auc</td><td>0.82074</td></tr><tr><td>iteration</td><td>94</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">brisk-sweep-40</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/c4s6d617\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/c4s6d617</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_015153-c4s6d617/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ps5p2o6a with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.9373057828195784\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 69\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.7251202737860085\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.4043212877477315\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 288\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 122\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 235\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 872\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 231\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_015215-ps5p2o6a</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/ps5p2o6a\" target=\"_blank\">wobbly-sweep-41</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=122, min_child_samples=288 will be ignored. Current value: min_data_in_leaf=122\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=122, min_child_samples=288 will be ignored. Current value: min_data_in_leaf=122\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=122, min_child_samples=288 will be ignored. Current value: min_data_in_leaf=122\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046840 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 69 rounds\n",
            "[100]\ttraining's binary_logloss: 0.422674\tvalid_1's binary_logloss: 0.506547\n",
            "Early stopping, best iteration is:\n",
            "[54]\ttraining's binary_logloss: 0.43822\tvalid_1's binary_logloss: 0.50443\n",
            "VALID AUC : 0.8311156302348888 ACC : 0.7379032258064516\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▄▃▃▃▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.7379</td></tr><tr><td>auc</td><td>0.83112</td></tr><tr><td>iteration</td><td>122</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">wobbly-sweep-41</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/ps5p2o6a\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/ps5p2o6a</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_015215-ps5p2o6a/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: t4zy9p3q with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.7779176808913062\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 71\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.5392601807139531\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.4182955913307728\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 24\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 122\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 83\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 199\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 873\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 205\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_015247-t4zy9p3q</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/t4zy9p3q\" target=\"_blank\">treasured-sweep-42</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=83, min_child_samples=122 will be ignored. Current value: min_data_in_leaf=83\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=83, min_child_samples=122 will be ignored. Current value: min_data_in_leaf=83\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=83, min_child_samples=122 will be ignored. Current value: min_data_in_leaf=83\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041071 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 71 rounds\n",
            "[100]\ttraining's binary_logloss: 0.42624\tvalid_1's binary_logloss: 0.513462\n",
            "Early stopping, best iteration is:\n",
            "[77]\ttraining's binary_logloss: 0.433379\tvalid_1's binary_logloss: 0.50571\n",
            "VALID AUC : 0.8296526045514727 ACC : 0.7580645161290323\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▅▅▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▅▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.75806</td></tr><tr><td>auc</td><td>0.82965</td></tr><tr><td>iteration</td><td>147</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">treasured-sweep-42</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/t4zy9p3q\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/t4zy9p3q</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_015247-t4zy9p3q/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: b0pumpk4 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.612703826411956\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 58\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.8547670989458493\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.8600288243110958\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 24\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 238\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 57\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 1384\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 123\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_015314-b0pumpk4</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/b0pumpk4\" target=\"_blank\">fine-sweep-43</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=238 will be ignored. Current value: min_data_in_leaf=10\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=238 will be ignored. Current value: min_data_in_leaf=10\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=10, min_child_samples=238 will be ignored. Current value: min_data_in_leaf=10\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048987 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 58 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[39]\ttraining's binary_logloss: 0.450391\tvalid_1's binary_logloss: 0.504979\n",
            "VALID AUC : 0.8311083151064715 ACC : 0.7513440860215054\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▆▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂</td></tr><tr><td>valid_1_binary_logloss</td><td>█▇▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.75134</td></tr><tr><td>auc</td><td>0.83111</td></tr><tr><td>iteration</td><td>56</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">fine-sweep-43</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/b0pumpk4\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/b0pumpk4</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_015314-b0pumpk4/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3ihwdge7 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.8787045935160598\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 78\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.8102464036282928\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.9037193318278244\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 85\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 65\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 246\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 1448\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 160\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_015330-3ihwdge7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/3ihwdge7\" target=\"_blank\">woven-sweep-44</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=65, min_child_samples=85 will be ignored. Current value: min_data_in_leaf=65\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=65, min_child_samples=85 will be ignored. Current value: min_data_in_leaf=65\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=65, min_child_samples=85 will be ignored. Current value: min_data_in_leaf=65\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050921 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 78 rounds\n",
            "[100]\ttraining's binary_logloss: 0.422681\tvalid_1's binary_logloss: 0.557729\n",
            "Early stopping, best iteration is:\n",
            "[22]\ttraining's binary_logloss: 0.45589\tvalid_1's binary_logloss: 0.517903\n",
            "VALID AUC : 0.8228422199951719 ACC : 0.7486559139784946\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>▇▅▅▄▄▄▂▁▁▂▂▂▁▁▂▂▂▃▃▄▄▅▅▅▅▆▆▆▆▆██████▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.74866</td></tr><tr><td>auc</td><td>0.82284</td></tr><tr><td>iteration</td><td>99</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">woven-sweep-44</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/3ihwdge7\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/3ihwdge7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_015330-3ihwdge7/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5zk3umht with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.9408194196152369\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 144\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.7151636525133569\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.6696337409432053\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 27\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 184\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 159\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 891\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 244\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_015352-5zk3umht</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/5zk3umht\" target=\"_blank\">rosy-sweep-45</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=184 will be ignored. Current value: min_data_in_leaf=32\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=184 will be ignored. Current value: min_data_in_leaf=32\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=32, min_child_samples=184 will be ignored. Current value: min_data_in_leaf=32\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056878 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 144 rounds\n",
            "[100]\ttraining's binary_logloss: 0.409764\tvalid_1's binary_logloss: 0.523018\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[159]\ttraining's binary_logloss: 0.387924\tvalid_1's binary_logloss: 0.533836\n",
            "VALID AUC : 0.8159221085126149 ACC : 0.7379032258064516\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▇▆▆▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▄▄▃▃▂▁▁▁▁▂▃▄▄▃▃▄▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▅▆▆▅▆▆▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.7379</td></tr><tr><td>auc</td><td>0.81592</td></tr><tr><td>iteration</td><td>158</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">rosy-sweep-45</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/5zk3umht\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/5zk3umht</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_015352-5zk3umht/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7qv5575n with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.624584168958938\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 81\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.962905196822315\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.916466485020314\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 26\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 373\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 81\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 244\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 982\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 93\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_015419-7qv5575n</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/7qv5575n\" target=\"_blank\">vibrant-sweep-46</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=81, min_child_samples=373 will be ignored. Current value: min_data_in_leaf=81\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=81, min_child_samples=373 will be ignored. Current value: min_data_in_leaf=81\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=81, min_child_samples=373 will be ignored. Current value: min_data_in_leaf=81\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054392 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 81 rounds\n",
            "[100]\ttraining's binary_logloss: 0.436257\tvalid_1's binary_logloss: 0.524972\n",
            "Early stopping, best iteration is:\n",
            "[56]\ttraining's binary_logloss: 0.447374\tvalid_1's binary_logloss: 0.510441\n",
            "VALID AUC : 0.8265583052310483 ACC : 0.739247311827957\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▇▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>██▅▅▄▄▃▃▃▃▃▃▃▃▂▂▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▆▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.73925</td></tr><tr><td>auc</td><td>0.82656</td></tr><tr><td>iteration</td><td>136</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">vibrant-sweep-46</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/7qv5575n\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/7qv5575n</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_015419-7qv5575n/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: rgdy9i37 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.815622518962047\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 166\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.8530763149734304\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.5148083177859034\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 190\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 54\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 1424\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 82\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_015449-rgdy9i37</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/rgdy9i37\" target=\"_blank\">glorious-sweep-47</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=190 will be ignored. Current value: min_data_in_leaf=12\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=190 will be ignored. Current value: min_data_in_leaf=12\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=12, min_child_samples=190 will be ignored. Current value: min_data_in_leaf=12\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048804 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 166 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[54]\ttraining's binary_logloss: 0.454009\tvalid_1's binary_logloss: 0.510802\n",
            "VALID AUC : 0.82587799828826 ACC : 0.7513440860215054\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▅▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▆▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.75134</td></tr><tr><td>auc</td><td>0.82588</td></tr><tr><td>iteration</td><td>53</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">glorious-sweep-47</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/rgdy9i37\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/rgdy9i37</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_015449-rgdy9i37/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: haorvu6i with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.6657536256655353\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 96\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.5179884819804441\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.7507296663793621\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 279\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 38\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 60\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 651\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 70\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_015506-haorvu6i</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/haorvu6i\" target=\"_blank\">cool-sweep-48</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=38, min_child_samples=279 will be ignored. Current value: min_data_in_leaf=38\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=38, min_child_samples=279 will be ignored. Current value: min_data_in_leaf=38\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=38, min_child_samples=279 will be ignored. Current value: min_data_in_leaf=38\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029616 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 96 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[60]\ttraining's binary_logloss: 0.456182\tvalid_1's binary_logloss: 0.510558\n",
            "VALID AUC : 0.826997212936073 ACC : 0.7567204301075269\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▆▅▅▅▄▅▅▅▅▄▄▃▃▃▃▃▃▂▂▂▂▂▁▂▁▂▂▁▁▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.75672</td></tr><tr><td>auc</td><td>0.827</td></tr><tr><td>iteration</td><td>59</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">cool-sweep-48</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/haorvu6i\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/haorvu6i</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_015506-haorvu6i/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xxvb184t with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.7973127458138596\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 188\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.5088929534263074\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.9050657925607704\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 356\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 101\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 280\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 992\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 136\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_015522-xxvb184t</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/xxvb184t\" target=\"_blank\">lilac-sweep-49</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=101, min_child_samples=356 will be ignored. Current value: min_data_in_leaf=101\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=101, min_child_samples=356 will be ignored. Current value: min_data_in_leaf=101\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=101, min_child_samples=356 will be ignored. Current value: min_data_in_leaf=101\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029014 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 188 rounds\n",
            "[100]\ttraining's binary_logloss: 0.427746\tvalid_1's binary_logloss: 0.526301\n",
            "[200]\ttraining's binary_logloss: 0.402983\tvalid_1's binary_logloss: 0.55284\n",
            "Early stopping, best iteration is:\n",
            "[48]\ttraining's binary_logloss: 0.444641\tvalid_1's binary_logloss: 0.513492\n",
            "VALID AUC : 0.8245173844026832 ACC : 0.7432795698924731\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▇▆▆▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>▆▃▄▂▁▂▂▁▁▁▁▁▁▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.74328</td></tr><tr><td>auc</td><td>0.82452</td></tr><tr><td>iteration</td><td>235</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">lilac-sweep-49</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/xxvb184t\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/xxvb184t</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_015522-xxvb184t/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xcrxnc12 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.5140286376845367\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 85\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.7659317838946427\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.5410436224287727\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 390\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 54\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 249\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 875\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 137\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_015553-xcrxnc12</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/xcrxnc12\" target=\"_blank\">spring-sweep-50</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=54, min_child_samples=390 will be ignored. Current value: min_data_in_leaf=54\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=54, min_child_samples=390 will be ignored. Current value: min_data_in_leaf=54\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=54, min_child_samples=390 will be ignored. Current value: min_data_in_leaf=54\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043032 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 85 rounds\n",
            "[100]\ttraining's binary_logloss: 0.430206\tvalid_1's binary_logloss: 0.494243\n",
            "Early stopping, best iteration is:\n",
            "[97]\ttraining's binary_logloss: 0.431032\tvalid_1's binary_logloss: 0.493672\n",
            "VALID AUC : 0.8410788351389509 ACC : 0.7540322580645161\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>training_binary_logloss</td><td>█▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▆▅▅▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▁▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.75403</td></tr><tr><td>auc</td><td>0.84108</td></tr><tr><td>iteration</td><td>181</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">spring-sweep-50</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/xcrxnc12\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/xcrxnc12</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_015553-xcrxnc12/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: d2b5inmq with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.516855510328976\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 196\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.7164786039430634\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.3908602291454788\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 372\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 17\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 337\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 1152\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 243\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_015621-d2b5inmq</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/d2b5inmq\" target=\"_blank\">neat-sweep-51</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=17, min_child_samples=372 will be ignored. Current value: min_data_in_leaf=17\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=17, min_child_samples=372 will be ignored. Current value: min_data_in_leaf=17\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=17, min_child_samples=372 will be ignored. Current value: min_data_in_leaf=17\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n",
            "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.219859 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 196 rounds\n",
            "[100]\ttraining's binary_logloss: 0.421666\tvalid_1's binary_logloss: 0.519336\n",
            "[200]\ttraining's binary_logloss: 0.39471\tvalid_1's binary_logloss: 0.520723\n",
            "Early stopping, best iteration is:\n",
            "[62]\ttraining's binary_logloss: 0.434796\tvalid_1's binary_logloss: 0.513706\n",
            "VALID AUC : 0.8242394095228341 ACC : 0.7473118279569892\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▇▆▆▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▆▃▃▂▁▂▂▁▁▂▂▂▁▂▂▂▂▃▃▂▂▂▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.74731</td></tr><tr><td>auc</td><td>0.82424</td></tr><tr><td>iteration</td><td>257</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">neat-sweep-51</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/d2b5inmq\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/d2b5inmq</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_015621-d2b5inmq/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tio4sg1z with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.5691178670700603\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 192\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.5834708728925962\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.5448995033398802\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 322\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 42\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 193\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 985\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 215\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_015707-tio4sg1z</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/tio4sg1z\" target=\"_blank\">crisp-sweep-52</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=42, min_child_samples=322 will be ignored. Current value: min_data_in_leaf=42\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=42, min_child_samples=322 will be ignored. Current value: min_data_in_leaf=42\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=42, min_child_samples=322 will be ignored. Current value: min_data_in_leaf=42\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034929 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 192 rounds\n",
            "[100]\ttraining's binary_logloss: 0.421646\tvalid_1's binary_logloss: 0.504623\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[193]\ttraining's binary_logloss: 0.39507\tvalid_1's binary_logloss: 0.511554\n",
            "VALID AUC : 0.8280652216849665 ACC : 0.7352150537634409\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▆▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▇▆▄▃▂▂▂▂▂▁▂▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.73522</td></tr><tr><td>auc</td><td>0.82807</td></tr><tr><td>iteration</td><td>192</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">crisp-sweep-52</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/tio4sg1z\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/tio4sg1z</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_015707-tio4sg1z/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: e1udf8d8 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.973553268000062\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 50\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.6959314554647489\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.6876083263753597\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 13\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 341\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 51\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 101\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 699\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 119\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_015734-e1udf8d8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/e1udf8d8\" target=\"_blank\">eager-sweep-53</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=51, min_child_samples=341 will be ignored. Current value: min_data_in_leaf=51\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=51, min_child_samples=341 will be ignored. Current value: min_data_in_leaf=51\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=51, min_child_samples=341 will be ignored. Current value: min_data_in_leaf=51\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058259 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "[100]\ttraining's binary_logloss: 0.432716\tvalid_1's binary_logloss: 0.499155\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[101]\ttraining's binary_logloss: 0.432398\tvalid_1's binary_logloss: 0.501195\n",
            "VALID AUC : 0.8363752075667689 ACC : 0.7634408602150538\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▅▅▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▆▅▅▅▅▄▄▄▃▃▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.76344</td></tr><tr><td>auc</td><td>0.83638</td></tr><tr><td>iteration</td><td>100</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">eager-sweep-53</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/e1udf8d8\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/e1udf8d8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_015734-e1udf8d8/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fjahnu7i with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.6129579163719262\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 144\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.5138391926320688\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.7396766372706822\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 249\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 87\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 82\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 1033\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 140\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_015755-fjahnu7i</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/fjahnu7i\" target=\"_blank\">likely-sweep-54</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=87, min_child_samples=249 will be ignored. Current value: min_data_in_leaf=87\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=87, min_child_samples=249 will be ignored. Current value: min_data_in_leaf=87\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=87, min_child_samples=249 will be ignored. Current value: min_data_in_leaf=87\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n",
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.039858 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 144 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[82]\ttraining's binary_logloss: 0.435008\tvalid_1's binary_logloss: 0.521841\n",
            "VALID AUC : 0.8199454291420086 ACC : 0.7473118279569892\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▅▅▅▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▆▅▄▄▄▄▂▂▁▁▁▁▁▁▂▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.74731</td></tr><tr><td>auc</td><td>0.81995</td></tr><tr><td>iteration</td><td>81</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">likely-sweep-54</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/fjahnu7i\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/fjahnu7i</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_015755-fjahnu7i/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ykh9rijx with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.5220628204930835\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 69\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.9887968949405086\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.8146771614504744\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 72\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 54\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 1391\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 95\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_015821-ykh9rijx</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/ykh9rijx\" target=\"_blank\">leafy-sweep-55</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=128, min_child_samples=72 will be ignored. Current value: min_data_in_leaf=128\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=128, min_child_samples=72 will be ignored. Current value: min_data_in_leaf=128\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=128, min_child_samples=72 will be ignored. Current value: min_data_in_leaf=128\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067637 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 69 rounds\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[54]\ttraining's binary_logloss: 0.470552\tvalid_1's binary_logloss: 0.52142\n",
            "VALID AUC : 0.8163171254471372 ACC : 0.7405913978494624\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▆▅▅▅▅▄▄▄▄▄▄▃▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.74059</td></tr><tr><td>auc</td><td>0.81632</td></tr><tr><td>iteration</td><td>53</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">leafy-sweep-55</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/ykh9rijx\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/ykh9rijx</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_015821-ykh9rijx/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tw6p42ke with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.866337406715227\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 134\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.6530233218687795\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.8389003428105963\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 14\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 253\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 53\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 100\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 763\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 213\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_015836-tw6p42ke</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/tw6p42ke\" target=\"_blank\">fine-sweep-56</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=53, min_child_samples=253 will be ignored. Current value: min_data_in_leaf=53\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=53, min_child_samples=253 will be ignored. Current value: min_data_in_leaf=53\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=53, min_child_samples=253 will be ignored. Current value: min_data_in_leaf=53\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041300 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 134 rounds\n",
            "[100]\ttraining's binary_logloss: 0.413455\tvalid_1's binary_logloss: 0.530849\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[100]\ttraining's binary_logloss: 0.413455\tvalid_1's binary_logloss: 0.530849\n",
            "VALID AUC : 0.81979181144525 ACC : 0.739247311827957\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁</td></tr><tr><td>auc</td><td>▁</td></tr><tr><td>iteration</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>training_binary_logloss</td><td>█▆▆▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>valid_1_binary_logloss</td><td>█▅▄▄▃▃▁▁▁▁▁▁▂▂▂▁▁▁▁▁▂▂▂▂▂▂▂▂▂▃▃▂▂▂▃▃▃▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.73925</td></tr><tr><td>auc</td><td>0.81979</td></tr><tr><td>iteration</td><td>99</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">fine-sweep-56</strong>: <a href=\"https://wandb.ai/recsys_01/LGBM/runs/tw6p42ke\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/runs/tw6p42ke</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20220502_015836-tw6p42ke/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: txjmsbdf with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbagging_fraction: 0.7824867482893259\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tearly_stopping_round: 129\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfeature_fraction: 0.8770739720130523\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.891891949671312\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_depth: 24\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_child_samples: 348\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_data_in_leaf: 125\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_estimators: 203\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_iterations: 1250\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_leaves: 109\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.12.15 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.14"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/opt/ml/wandb/run-20220502_015858-txjmsbdf</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/recsys_01/LGBM/runs/txjmsbdf\" target=\"_blank\">blooming-sweep-57</a></strong> to <a href=\"https://wandb.ai/recsys_01/LGBM\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x\" target=\"_blank\">https://wandb.ai/recsys_01/LGBM/sweeps/hhobp47x</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
            "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/opt/conda/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=125, min_child_samples=348 will be ignored. Current value: min_data_in_leaf=125\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=25, min_child_samples=281 will be ignored. Current value: min_data_in_leaf=25\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=125, min_child_samples=348 will be ignored. Current value: min_data_in_leaf=125\n",
            "[LightGBM] [Warning] min_data_in_leaf is set=125, min_child_samples=348 will be ignored. Current value: min_data_in_leaf=125\n",
            "[LightGBM] [Info] Number of positive: 1483205, number of negative: 783381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'max_depth' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_child_samples' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_leaves' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'bagging_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'feature_fraction' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'min_data_in_leaf' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'num_iterations' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'early_stopping_round' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056652 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 7676\n",
            "[LightGBM] [Info] Number of data points in the train set: 2266586, number of used features: 40\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.654378 -> initscore=0.638341\n",
            "[LightGBM] [Info] Start training from score 0.638341\n",
            "Training until validation scores don't improve for 129 rounds\n",
            "[100]\ttraining's binary_logloss: 0.432366\tvalid_1's binary_logloss: 0.512334\n",
            "Early stopping, best iteration is:\n",
            "[43]\ttraining's binary_logloss: 0.449611\tvalid_1's binary_logloss: 0.495254\n",
            "VALID AUC : 0.8390232840537515 ACC : 0.7580645161290323\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Problem finishing run\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/conda/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 1874, in _atexit_cleanup\n",
            "    self._on_finish()\n",
            "  File \"/opt/conda/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 1999, in _on_finish\n",
            "    time.sleep(0.1)\n",
            "Exception\n"
          ]
        }
      ],
      "source": [
        "wandb.agent(sweep_id, train) # count : 실험 횟수"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Bsff1ZVhfQ3"
      },
      "source": [
        "## 5. Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_ = lgb.plot_importance(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-05-24T09:49:29.691738Z",
          "start_time": "2021-05-24T09:49:28.992Z"
        },
        "id": "N6YEFm8IhfQ3"
      },
      "outputs": [],
      "source": [
        "# test_df\n",
        "display(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(FEATS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-05-24T09:49:29.692739Z",
          "start_time": "2021-05-24T09:49:28.993Z"
        },
        "id": "XnwXJs_shfQ4"
      },
      "outputs": [],
      "source": [
        "# MAKE PREDICTION\n",
        "total_preds = model.predict(test[FEATS])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-05-24T09:49:29.694736Z",
          "start_time": "2021-05-24T09:49:28.995Z"
        },
        "id": "f8PvohzwhfQ4"
      },
      "outputs": [],
      "source": [
        "# SAVE OUTPUT\n",
        "output_dir = '/opt/ml/'\n",
        "write_path = os.path.join(output_dir, \"submission_lgbm_fe_40_220502.csv\")\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "with open(write_path, 'w', encoding='utf8') as w:\n",
        "    print(\"writing prediction : {}\".format(write_path))\n",
        "    w.write(\"id,prediction\\n\")\n",
        "    for id, p in enumerate(total_preds):\n",
        "        w.write('{},{}\\n'.format(id,p))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "(미션-2) LGBM Baseline.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
